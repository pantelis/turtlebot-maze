{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:28:09Z","event_type":"created","id":1,"issue_id":"turtlebot-maze-8eg","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:29:01Z","event_type":"status_changed","id":2,"issue_id":"turtlebot-maze-8eg","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-8eg\",\"title\":\"Dev container workspace path /workspaces/turtlebot-maze does not exist\",\"description\":\"When launching the Docker dev container via VS Code (Dev Container: Existing Docker Compose), two errors occur: (1) VS Code dialog: Workspace does not exist - Please select another workspace to open. (2) Terminal error: The terminal process failed to launch: Starting directory (cwd) /workspaces/turtlebot-maze does not exist. The devcontainer.json or docker-compose config references /workspaces/turtlebot-maze as the workspace folder, but that path is not created or mounted inside the container. Fix: Ensure the volume mount in docker-compose.yml maps the project source to /workspaces/turtlebot-maze, or update devcontainer.json workspaceFolder to match the actual mount point.\",\"status\":\"open\",\"priority\":1,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-19T13:28:10Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-19T13:28:10Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:29:34Z","event_type":"closed","id":3,"issue_id":"turtlebot-maze-8eg","new_value":"Fixed by: (1) Adding volume mount '../:/workspaces/turtlebot-maze:cached' in .devcontainer/docker-compose.yml so the workspace path exists inside the container. (2) Switching service from 'demo-world' to 'dev' in devcontainer.json since 'dev' is the proper development service with sleep infinity and user permissions.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:33:26Z","event_type":"reopened","id":4,"issue_id":"turtlebot-maze-8eg","new_value":"{\"status\":\"open\"}","old_value":"{\"id\":\"turtlebot-maze-8eg\",\"title\":\"Dev container workspace path /workspaces/turtlebot-maze does not exist\",\"description\":\"When launching the Docker dev container via VS Code (Dev Container: Existing Docker Compose), two errors occur: (1) VS Code dialog: Workspace does not exist - Please select another workspace to open. (2) Terminal error: The terminal process failed to launch: Starting directory (cwd) /workspaces/turtlebot-maze does not exist. The devcontainer.json or docker-compose config references /workspaces/turtlebot-maze as the workspace folder, but that path is not created or mounted inside the container. Fix: Ensure the volume mount in docker-compose.yml maps the project source to /workspaces/turtlebot-maze, or update devcontainer.json workspaceFolder to match the actual mount point.\",\"status\":\"closed\",\"priority\":1,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-19T13:28:10Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-19T13:29:35Z\",\"closed_at\":\"2026-02-19T13:29:35Z\",\"close_reason\":\"Fixed by: (1) Adding volume mount '../:/workspaces/turtlebot-maze:cached' in .devcontainer/docker-compose.yml so the workspace path exists inside the container. (2) Switching service from 'demo-world' to 'dev' in devcontainer.json since 'dev' is the proper development service with sleep infinity and user permissions.\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:34:56Z","event_type":"closed","id":5,"issue_id":"turtlebot-maze-8eg","new_value":"Fixed volume mount path resolution. The override compose file used '..' which resolved relative to the primary docker-compose.yaml (project root), going one level too high to /navigation/ instead of /turtlebot-maze/. Changed to '.' which correctly resolves to the project root. Also switched service to 'dev' and removed obsolete version key.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:41:01Z","event_type":"updated","id":6,"issue_id":"turtlebot-maze-8eg","new_value":"{\"notes\":\"Root cause: Two issues - (1) devcontainer.json pointed to wrong service (demo-world → dev) and volume mount path was incorrect, (2) zenoh-bridge-ros2dds v1.2.1 .deb no longer exists on GitHub (repo renamed to zenoh-plugin-ros2dds, releases now ship as zip archives). Fix: Updated .devcontainer files to use dev service with correct mount, and updated Dockerfile.gpu to use zenoh-plugin-ros2dds v1.7.2 with new zip-based download format.\"}","old_value":"{\"id\":\"turtlebot-maze-8eg\",\"title\":\"Dev container workspace path /workspaces/turtlebot-maze does not exist\",\"description\":\"When launching the Docker dev container via VS Code (Dev Container: Existing Docker Compose), two errors occur: (1) VS Code dialog: Workspace does not exist - Please select another workspace to open. (2) Terminal error: The terminal process failed to launch: Starting directory (cwd) /workspaces/turtlebot-maze does not exist. The devcontainer.json or docker-compose config references /workspaces/turtlebot-maze as the workspace folder, but that path is not created or mounted inside the container. Fix: Ensure the volume mount in docker-compose.yml maps the project source to /workspaces/turtlebot-maze, or update devcontainer.json workspaceFolder to match the actual mount point.\",\"status\":\"closed\",\"priority\":1,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-19T13:28:10Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-19T13:34:56Z\",\"closed_at\":\"2026-02-19T13:34:56Z\",\"close_reason\":\"Fixed volume mount path resolution. The override compose file used '..' which resolved relative to the primary docker-compose.yaml (project root), going one level too high to /navigation/ instead of /turtlebot-maze/. Changed to '.' which correctly resolves to the project root. Also switched service to 'dev' and removed obsolete version key.\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:41:05Z","event_type":"closed","id":7,"issue_id":"turtlebot-maze-8eg","new_value":"Fixed devcontainer service reference and zenoh-bridge version/download format in Dockerfile.gpu","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:51:49Z","event_type":"reopened","id":8,"issue_id":"turtlebot-maze-8eg","new_value":"{\"status\":\"open\"}","old_value":"{\"id\":\"turtlebot-maze-8eg\",\"title\":\"Dev container workspace path /workspaces/turtlebot-maze does not exist\",\"description\":\"When launching the Docker dev container via VS Code (Dev Container: Existing Docker Compose), two errors occur: (1) VS Code dialog: Workspace does not exist - Please select another workspace to open. (2) Terminal error: The terminal process failed to launch: Starting directory (cwd) /workspaces/turtlebot-maze does not exist. The devcontainer.json or docker-compose config references /workspaces/turtlebot-maze as the workspace folder, but that path is not created or mounted inside the container. Fix: Ensure the volume mount in docker-compose.yml maps the project source to /workspaces/turtlebot-maze, or update devcontainer.json workspaceFolder to match the actual mount point.\",\"notes\":\"Root cause: Two issues - (1) devcontainer.json pointed to wrong service (demo-world → dev) and volume mount path was incorrect, (2) zenoh-bridge-ros2dds v1.2.1 .deb no longer exists on GitHub (repo renamed to zenoh-plugin-ros2dds, releases now ship as zip archives). Fix: Updated .devcontainer files to use dev service with correct mount, and updated Dockerfile.gpu to use zenoh-plugin-ros2dds v1.7.2 with new zip-based download format.\",\"status\":\"closed\",\"priority\":1,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-19T13:28:10Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-19T13:41:05Z\",\"closed_at\":\"2026-02-19T13:41:05Z\",\"close_reason\":\"Fixed devcontainer service reference and zenoh-bridge version/download format in Dockerfile.gpu\"}"}
{"actor":"Pantelis Monogioudis","comment":"Network unreachable fetching devcontainer features from ghcr.io - git-lfs and python features redundant with Dockerfile","created_at":"2026-02-19T08:51:49Z","event_type":"commented","id":9,"issue_id":"turtlebot-maze-8eg","new_value":null,"old_value":null}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:52:02Z","event_type":"closed","id":10,"issue_id":"turtlebot-maze-8eg","new_value":"Removed ghcr.io devcontainer features (git-lfs, python) that fail when network is unreachable — both already provided by Dockerfile.gpu","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T08:58:57Z","event_type":"closed","id":11,"issue_id":"turtlebot-maze-8eg","new_value":"All three issues fixed and verified: (1) devcontainer service pointed to dev, (2) zenoh-bridge updated to v1.7.2 from new repo, (3) only install bridge deb to avoid zenohd dependency. Docker build succeeds and container starts with correct workspace mount.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T09:13:14Z","event_type":"created","id":12,"issue_id":"turtlebot-maze-dwg","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T09:33:08Z","event_type":"closed","id":13,"issue_id":"turtlebot-maze-dwg","new_value":"Demo verified end-to-end: /navigate slash command connects to rosbridge, verifies Nav2, sends NavigateToPose goal to location3, robot arrives within 0.1m of target","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T09:53:59Z","event_type":"created","id":14,"issue_id":"turtlebot-maze-uqk","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T11:10:54Z","event_type":"status_changed","id":15,"issue_id":"turtlebot-maze-uqk","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-uqk\",\"title\":\"Investigate missing RealSense camera frames in RViz2\",\"description\":\"Cannot see frames from the RealSense camera in RViz2. Need to diagnose why — likely a TF, topic, or driver configuration issue.\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-19T14:53:59Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-19T14:53:59Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T11:13:52Z","event_type":"updated","id":16,"issue_id":"turtlebot-maze-uqk","new_value":"{\"notes\":\"PR #7 created: https://github.com/pantelis/turtlebot-maze/pull/7. Root cause: Gazebo sensor was type=camera (no depth output) and bridge mapped to wrong ROS topic names. Fix: changed to depth_camera + corrected bridge mappings.\"}","old_value":"{\"id\":\"turtlebot-maze-uqk\",\"title\":\"Investigate missing RealSense camera frames in RViz2\",\"description\":\"Cannot see frames from the RealSense camera in RViz2. Need to diagnose why — likely a TF, topic, or driver configuration issue.\",\"status\":\"in_progress\",\"priority\":2,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-19T14:53:59Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-19T16:10:54Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T11:45:53Z","event_type":"updated","id":17,"issue_id":"turtlebot-maze-uqk","new_value":"{\"notes\":\"Fix verified: all 3 ROS topics (/intel_realsense_r200_depth/image_raw, /points, /camera/image_raw) now have data flowing. Root cause was sensor type=camera (needed type=depth) and bridge subscribed to wrong Gazebo topic names.\"}","old_value":"{\"id\":\"turtlebot-maze-uqk\",\"title\":\"Investigate missing RealSense camera frames in RViz2\",\"description\":\"Cannot see frames from the RealSense camera in RViz2. Need to diagnose why — likely a TF, topic, or driver configuration issue.\",\"notes\":\"PR #7 created: https://github.com/pantelis/turtlebot-maze/pull/7. Root cause: Gazebo sensor was type=camera (no depth output) and bridge mapped to wrong ROS topic names. Fix: changed to depth_camera + corrected bridge mappings.\",\"status\":\"in_progress\",\"priority\":2,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-19T14:53:59Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-19T16:13:52Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T11:52:04Z","event_type":"updated","id":18,"issue_id":"turtlebot-maze-uqk","new_value":"{\"notes\":\"Fix verified: all 3 ROS topics (/intel_realsense_r200_depth/image_raw, /points, /camera/image_raw) now have data flowing at ~5Hz. Root cause was sensor type=camera (needed type=depth) and bridge subscribed to wrong Gazebo topic names. The depth_image subtopic does not publish with type=depth, but the base topic carries 32FC1 depth data which is sufficient.\"}","old_value":"{\"id\":\"turtlebot-maze-uqk\",\"title\":\"Investigate missing RealSense camera frames in RViz2\",\"description\":\"Cannot see frames from the RealSense camera in RViz2. Need to diagnose why — likely a TF, topic, or driver configuration issue.\",\"notes\":\"Fix verified: all 3 ROS topics (/intel_realsense_r200_depth/image_raw, /points, /camera/image_raw) now have data flowing. Root cause was sensor type=camera (needed type=depth) and bridge subscribed to wrong Gazebo topic names.\",\"status\":\"in_progress\",\"priority\":2,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-19T14:53:59Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-19T16:45:53Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-19T11:52:09Z","event_type":"closed","id":19,"issue_id":"turtlebot-maze-uqk","new_value":"Fixed: changed sensor type from camera to depth (matching upstream nav2_minimal_tb3_sim), corrected bridge topic mapping to use base Gazebo topic for images. PR #7 verified with data flowing on all expected topics at 5Hz.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-20T11:47:38Z","event_type":"created","id":20,"issue_id":"turtlebot-maze-4y3","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-20T14:44:22Z","event_type":"status_changed","id":21,"issue_id":"turtlebot-maze-4y3","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-4y3\",\"title\":\"Author tutorial blog post: Enhancing the TurtleBot Maze World\",\"description\":\"Write a Mintlify blog post tutorial covering all steps to enhance the maze world: cherry-picking assets from PR #3, creating sim_house_enhanced.sdf.xacro with taller walls and PBR textures, adding ArUco marker spawner, parameterizing launch files with world_name/use_aruco, and adding demo-world-enhanced Docker Compose service. Target location: /home/pantelis.monogioudis/local/web/sites/courses/eaia/src/blog/tutorials/\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-20T16:47:38Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-20T16:47:38Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-20T14:47:50Z","event_type":"closed","id":22,"issue_id":"turtlebot-maze-4y3","new_value":"Tutorial blog post authored at blog/tutorials/enhancing-turtlebot-maze/index.mdx","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-20T15:02:00Z","event_type":"reopened","id":23,"issue_id":"turtlebot-maze-4y3","new_value":"{\"status\":\"open\"}","old_value":"{\"id\":\"turtlebot-maze-4y3\",\"title\":\"Author tutorial blog post: Enhancing the TurtleBot Maze World\",\"description\":\"Write a Mintlify blog post tutorial covering all steps to enhance the maze world: cherry-picking assets from PR #3, creating sim_house_enhanced.sdf.xacro with taller walls and PBR textures, adding ArUco marker spawner, parameterizing launch files with world_name/use_aruco, and adding demo-world-enhanced Docker Compose service. Target location: /home/pantelis.monogioudis/local/web/sites/courses/eaia/src/blog/tutorials/\",\"status\":\"closed\",\"priority\":2,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-20T16:47:38Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-20T19:47:50Z\",\"closed_at\":\"2026-02-20T19:47:50Z\",\"close_reason\":\"Tutorial blog post authored at blog/tutorials/enhancing-turtlebot-maze/index.mdx\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-20T15:02:04Z","event_type":"status_changed","id":24,"issue_id":"turtlebot-maze-4y3","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-4y3\",\"title\":\"Author tutorial blog post: Enhancing the TurtleBot Maze World\",\"description\":\"Write a Mintlify blog post tutorial covering all steps to enhance the maze world: cherry-picking assets from PR #3, creating sim_house_enhanced.sdf.xacro with taller walls and PBR textures, adding ArUco marker spawner, parameterizing launch files with world_name/use_aruco, and adding demo-world-enhanced Docker Compose service. Target location: /home/pantelis.monogioudis/local/web/sites/courses/eaia/src/blog/tutorials/\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-20T16:47:38Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-20T20:02:00Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-20T15:05:38Z","event_type":"closed","id":25,"issue_id":"turtlebot-maze-4y3","new_value":"Tutorial rewritten: starts from original featureless maze, walks through generic enhancement steps, no PR references. Pushed to eaia PR #46.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T06:54:08Z","event_type":"created","id":26,"issue_id":"turtlebot-maze-5k3","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T06:54:25Z","event_type":"updated","id":27,"issue_id":"turtlebot-maze-5k3","new_value":"{\"design\":\"# ROS 2 Documentation MCP Server — Design Outline\\n\\n## Problem Statement\\nAI coding assistants (Claude Code, Cursor, etc.) connected to ROS 2 systems via ros-mcp-server can introspect running robots but cannot consult ROS 2 documentation. When a roboticist needs to verify API behavior — e.g., how ExecuteProcess.cmd handles LaunchConfiguration substitutions, or what arguments gz_spawn_model.launch.py accepts — the assistant must fall back to web search or reading raw source code. This is slow, unreliable, and breaks the workflow.\\n\\n## Architecture\\n- Python MCP server using FastMCP or the official MCP Python SDK\\n- Documentation ingested from public sources, chunked and indexed\\n- Vector or keyword search over indexed docs\\n- Tools exposed: search_ros2_docs, get_api_reference, get_message_definition, get_launch_file_args\\n\\n## Documentation Sources (Priority Order)\\n1. **ROS 2 Launch API** — docs.ros.org/en/jazzy/p/launch/\\n2. **rclpy API** — docs.ros.org/en/jazzy/p/rclpy/\\n3. **Nav2 docs** — docs.nav2.org\\n4. **SDFormat spec** — sdformat.org\\n5. **Gazebo Sim API** — gazebosim.org/api\\n6. **ROS 2 tutorials** — docs.ros.org/en/jazzy/Tutorials/\\n\\n## Existing Options Survey\\n| Option | Pros | Cons |\\n|--------|------|------|\\n| Context7 MCP | Ready-made, curated DB, Claude plugin | May not cover ROS 2; proprietary index |\\n| Library Docs MCP (vikramdse) | Generic, uses Serper search | External API dependency; not ROS-specific |\\n| Google Dev Knowledge MCP | Good pattern reference | Google docs only |\\n| Custom build | Full control, offline-capable, ROS 2-specific | Build effort |\\n\\n## Implementation Plan\\n### Phase 1: Evaluate Context7 coverage\\n- Install Context7 MCP server\\n- Test if ROS 2 launch, rclpy, Nav2 docs are indexed\\n- If coverage is sufficient, document setup and skip custom build\\n\\n### Phase 2: Custom MCP server (if needed)\\n- Scrape/download ROS 2 doc pages (HTML → markdown)\\n- Chunk and index with lightweight search (e.g., tantivy, sqlite-fts5, or embedded vectors)\\n- Implement MCP tools: search_ros2_docs, get_api_reference\\n- Package as pip-installable, add to Claude Code via claude mcp add\\n\\n### Phase 3: Blog tutorial\\n- Write tutorial in eaia repo: src/blog/tutorials/ros2-docs-mcp/\\n- Document the problem (roboticists + AI assistants + missing docs)\\n- Survey all options (Context7, Library Docs MCP, Google Dev Knowledge, custom)\\n- Show working setup with Claude Code\\n- Include practical examples (verifying launch API behavior, looking up message types)\"}","old_value":"{\"id\":\"turtlebot-maze-5k3\",\"title\":\"Build ROS 2 Documentation MCP Server\",\"description\":\"Roboticists using Claude Code (or any MCP-compatible AI assistant) lack a documentation MCP server for ROS 2, Gazebo Sim, and Nav2. When verifying API behavior (e.g., how ExecuteProcess handles nested lists, what launch arguments gz_spawn_model accepts), the only options are web search or reading installed source — neither is reliable or fast.\\n\\n## Goal\\nBuild an MCP server that indexes and serves ROS 2 ecosystem documentation, enabling AI assistants to look up API references, launch file parameters, message types, and library behavior on demand.\\n\\n## Scope\\n- Index docs.ros.org (launch API, rclpy, rclcpp, message/service/action types)\\n- Index Gazebo Sim API docs (gz-sim, SDFormat, gz-transport)\\n- Index Nav2 documentation (configuration, plugins, behavior trees)\\n- Expose tools: search-docs, get-api-reference, get-message-type, get-launch-args\\n- Package as installable MCP server for Claude Code\\n\\n## Existing Options to Evaluate\\n1. **Context7 MCP** (github.com/upstash/context7) — general-purpose library doc server, may already cover some ROS 2 libs\\n2. **Library Docs MCP Server** (vikramdse/docs-mcp-server) — generic doc search via Serper API\\n3. **Google Developer Knowledge MCP** — pattern reference for doc-serving MCP\\n4. **Custom build** — scrape/index ROS 2 doc sites, serve via MCP tools\\n\\n## Deliverables\\n1. Working ROS 2 docs MCP server (MVP: launch API + Nav2 config docs)\\n2. Integration test with Claude Code\\n3. Blog tutorial in eaia repo (src/blog/tutorials/) documenting the problem, surveying existing options, and showing the solution\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"feature\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-21T11:54:08Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-21T11:54:08Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T06:55:58Z","event_type":"updated","id":28,"issue_id":"turtlebot-maze-5k3","new_value":"{\"notes\":\"# Landscape: Documentation MCP Servers for Roboticists\\n\\n## The Gap\\n\\nRoboticists using Claude Code with ros-mcp-server can introspect a running ROS 2 system (topics, services, actions, parameters) but have no way to consult API documentation. When verifying API behavior — e.g., how `ExecuteProcess.cmd` handles nested lists with `LaunchConfiguration` substitutions, or what arguments `gz_spawn_model.launch.py` accepts — the only options are:\\n\\n1. **Web search** — unreliable, slow, may return outdated results\\n2. **Reading installed source code** — accurate but requires knowing where to look\\n3. **Claude's training data** — covers ROS 2 through Jazzy but can be stale or wrong on edge cases\\n\\nNone of these are fast or integrated into the AI assistant workflow.\\n\\n## Existing Options\\n\\n### 1. Context7 MCP Server (Most Promising)\\n- **Repo**: https://github.com/upstash/context7\\n- **What it does**: Serves up-to-date, version-specific library documentation directly into the LLM context window. Pulls docs and code examples from a curated database.\\n- **Install for Claude Code**: `claude mcp add --scope user context7 -- npx -y @upstash/context7-mcp`\\n- **Tools**: `resolve-library-id` (find library), `query-docs` (fetch docs for a library)\\n- **ROS 2 coverage**: Unknown — needs evaluation. Libraries can potentially be added to their index.\\n- **Pros**: Ready-made, curated, version-aware, Claude plugin available\\n- **Cons**: Proprietary index; may not cover ROS 2 ecosystem; requires npm/npx\\n\\n### 2. Library Docs MCP Server\\n- **Registry**: https://lobehub.com/mcp/vikramdse-docs-mcp-server\\n- **What it does**: Generic MCP server for searching and fetching documentation for popular libraries using the Serper API.\\n- **Pros**: Generic approach works for any library with online docs\\n- **Cons**: External API dependency (Serper); not ROS-specific; search quality depends on web indexing\\n\\n### 3. Google Developer Knowledge MCP\\n- **Announcement**: https://developers.googleblog.com/introducing-the-developer-knowledge-api-and-mcp-server/\\n- **What it does**: Serves Google's official developer documentation via MCP. Provides a canonical, machine-readable gateway.\\n- **Pros**: Shows the gold-standard pattern for doc-serving MCP; structured API\\n- **Cons**: Google docs only — not applicable to ROS 2 directly; useful as architecture reference\\n\\n### 4. Custom ROS 2 Docs MCP Server (Build It)\\n- **What it would do**: Scrape/index docs.ros.org, Nav2, Gazebo Sim, SDFormat docs. Expose search and lookup tools via MCP.\\n- **Target doc sources**: launch API, rclpy, rclcpp, message types, Nav2 config, SDFormat spec, Gazebo Sim API\\n- **Tools**: `search_ros2_docs`, `get_api_reference`, `get_message_definition`, `get_launch_file_args`\\n- **Pros**: Full control, offline-capable, ROS 2-specific, covers the full ecosystem\\n- **Cons**: Build and maintenance effort; need to handle doc updates across ROS 2 releases\\n\\n## Recommendation\\n\\nStart with Phase 1: evaluate Context7 coverage of ROS 2. If it covers rclpy/launch/Nav2, document setup and skip custom build. If not, proceed to Phase 2 (custom build) using Google Dev Knowledge MCP as architecture reference.\"}","old_value":"{\"id\":\"turtlebot-maze-5k3\",\"title\":\"Build ROS 2 Documentation MCP Server\",\"description\":\"Roboticists using Claude Code (or any MCP-compatible AI assistant) lack a documentation MCP server for ROS 2, Gazebo Sim, and Nav2. When verifying API behavior (e.g., how ExecuteProcess handles nested lists, what launch arguments gz_spawn_model accepts), the only options are web search or reading installed source — neither is reliable or fast.\\n\\n## Goal\\nBuild an MCP server that indexes and serves ROS 2 ecosystem documentation, enabling AI assistants to look up API references, launch file parameters, message types, and library behavior on demand.\\n\\n## Scope\\n- Index docs.ros.org (launch API, rclpy, rclcpp, message/service/action types)\\n- Index Gazebo Sim API docs (gz-sim, SDFormat, gz-transport)\\n- Index Nav2 documentation (configuration, plugins, behavior trees)\\n- Expose tools: search-docs, get-api-reference, get-message-type, get-launch-args\\n- Package as installable MCP server for Claude Code\\n\\n## Existing Options to Evaluate\\n1. **Context7 MCP** (github.com/upstash/context7) — general-purpose library doc server, may already cover some ROS 2 libs\\n2. **Library Docs MCP Server** (vikramdse/docs-mcp-server) — generic doc search via Serper API\\n3. **Google Developer Knowledge MCP** — pattern reference for doc-serving MCP\\n4. **Custom build** — scrape/index ROS 2 doc sites, serve via MCP tools\\n\\n## Deliverables\\n1. Working ROS 2 docs MCP server (MVP: launch API + Nav2 config docs)\\n2. Integration test with Claude Code\\n3. Blog tutorial in eaia repo (src/blog/tutorials/) documenting the problem, surveying existing options, and showing the solution\",\"design\":\"# ROS 2 Documentation MCP Server — Design Outline\\n\\n## Problem Statement\\nAI coding assistants (Claude Code, Cursor, etc.) connected to ROS 2 systems via ros-mcp-server can introspect running robots but cannot consult ROS 2 documentation. When a roboticist needs to verify API behavior — e.g., how ExecuteProcess.cmd handles LaunchConfiguration substitutions, or what arguments gz_spawn_model.launch.py accepts — the assistant must fall back to web search or reading raw source code. This is slow, unreliable, and breaks the workflow.\\n\\n## Architecture\\n- Python MCP server using FastMCP or the official MCP Python SDK\\n- Documentation ingested from public sources, chunked and indexed\\n- Vector or keyword search over indexed docs\\n- Tools exposed: search_ros2_docs, get_api_reference, get_message_definition, get_launch_file_args\\n\\n## Documentation Sources (Priority Order)\\n1. **ROS 2 Launch API** — docs.ros.org/en/jazzy/p/launch/\\n2. **rclpy API** — docs.ros.org/en/jazzy/p/rclpy/\\n3. **Nav2 docs** — docs.nav2.org\\n4. **SDFormat spec** — sdformat.org\\n5. **Gazebo Sim API** — gazebosim.org/api\\n6. **ROS 2 tutorials** — docs.ros.org/en/jazzy/Tutorials/\\n\\n## Existing Options Survey\\n| Option | Pros | Cons |\\n|--------|------|------|\\n| Context7 MCP | Ready-made, curated DB, Claude plugin | May not cover ROS 2; proprietary index |\\n| Library Docs MCP (vikramdse) | Generic, uses Serper search | External API dependency; not ROS-specific |\\n| Google Dev Knowledge MCP | Good pattern reference | Google docs only |\\n| Custom build | Full control, offline-capable, ROS 2-specific | Build effort |\\n\\n## Implementation Plan\\n### Phase 1: Evaluate Context7 coverage\\n- Install Context7 MCP server\\n- Test if ROS 2 launch, rclpy, Nav2 docs are indexed\\n- If coverage is sufficient, document setup and skip custom build\\n\\n### Phase 2: Custom MCP server (if needed)\\n- Scrape/download ROS 2 doc pages (HTML → markdown)\\n- Chunk and index with lightweight search (e.g., tantivy, sqlite-fts5, or embedded vectors)\\n- Implement MCP tools: search_ros2_docs, get_api_reference\\n- Package as pip-installable, add to Claude Code via claude mcp add\\n\\n### Phase 3: Blog tutorial\\n- Write tutorial in eaia repo: src/blog/tutorials/ros2-docs-mcp/\\n- Document the problem (roboticists + AI assistants + missing docs)\\n- Survey all options (Context7, Library Docs MCP, Google Dev Knowledge, custom)\\n- Show working setup with Claude Code\\n- Include practical examples (verifying launch API behavior, looking up message types)\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"feature\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-21T11:54:08Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-21T11:54:25Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T07:24:46Z","event_type":"created","id":29,"issue_id":"turtlebot-maze-iyw","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T07:25:12Z","event_type":"status_changed","id":30,"issue_id":"turtlebot-maze-iyw","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-iyw\",\"title\":\"Fix broken Mermaid diagram in README System Overview\",\"description\":\"The System Overview section in README.md contains a Mermaid diagram that fails to render on GitHub with a lexical error:\\n\\n```\\nLexical error on line 3. Unrecognized text.\\n...AM[/camera/image_raw] NAV[Nav2 S\\n-----------------------^\\n```\\n\\nGitHub's Mermaid renderer is choking on the diagram syntax — likely special characters in node labels (forward slashes in topic names like `/camera/image_raw`, or spaces/special chars in labels). Need to fix the Mermaid syntax so it renders correctly on GitHub.\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-21T12:24:46Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-21T12:24:46Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T07:26:28Z","event_type":"closed","id":31,"issue_id":"turtlebot-maze-iyw","new_value":"Fixed in PR #10 — quoted all special characters in Mermaid node labels and subgraph titles. Auto-merge enabled.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T07:36:45Z","event_type":"created","id":32,"issue_id":"turtlebot-maze-m5e","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T07:55:09Z","event_type":"status_changed","id":33,"issue_id":"turtlebot-maze-m5e","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-m5e\",\"title\":\"Test stella_vslam Visual SLAM with enhanced maze world\",\"description\":\"PR #3 (oscarpoudel:updated_maze_plus_slam) includes an install script for stella_vslam at `docker/install_stella_vslam.sh`. The claim is that stella_vslam can run Visual SLAM on the TurtleBot in the maze world. We need to evaluate and integrate this.\\n\\n## Background\\n- [stella_vslam](https://github.com/stella-cv/stella_vslam) is a Visual SLAM framework (fork of OpenVSLAM)\\n- PR #3 includes a Jazzy-compatible ROS 2 wrapper: `oscarpoudel/stella_vslam_ros2_Jazzy`\\n- The install script builds: iridescence (viewer), stella_vslam (core), iridescence_viewer, and the ROS 2 wrapper\\n- The enhanced maze world (`demo-world-enhanced`) with textured walls should provide better visual features for VSLAM\\n\\n## Tasks\\n\\n### 1. Cherry-pick and evaluate the install script\\n```bash\\ngit checkout pr3-temp -- docker/install_stella_vslam.sh\\n```\\n- Review the script for correctness and security (hardcoded paths to `~/lib`, `~/ros2_ws`)\\n- Check if `oscarpoudel/stella_vslam_ros2_Jazzy` repo exists and is maintained\\n- Check stella_vslam compatibility with ROS 2 Jazzy and gz-sim\\n\\n### 2. Dockerize the build\\n- The install script uses `~/lib` and `~/ros2_ws` — needs adaptation for our Docker multi-stage build\\n- Consider a new Dockerfile stage or separate Dockerfile for stella_vslam\\n- stella_vslam needs camera images — verify the Gazebo camera topic is compatible\\n\\n### 3. Test with enhanced maze world\\n- Launch `demo-world-enhanced` (textured walls provide visual features for VSLAM)\\n- Run stella_vslam with the TurtleBot's camera feed\\n- Verify it can build a map and localize\\n- Compare SLAM performance: original featureless walls vs enhanced textured walls\\n\\n### 4. Add Docker Compose service\\n- Add `demo-slam` or similar service if stella_vslam works\\n- Document launch procedure in README\\n\\n## References\\n- PR #3: https://github.com/pantelis/turtlebot-maze/pull/3\\n- stella_vslam: https://github.com/stella-cv/stella_vslam\\n- Jazzy wrapper (claimed): https://github.com/oscarpoudel/stella_vslam_ros2_Jazzy\\n- Install script in PR #3: `docker/install_stella_vslam.sh`\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"feature\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-21T12:36:45Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-21T12:36:45Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T08:11:07Z","event_type":"closed","id":34,"issue_id":"turtlebot-maze-m5e","new_value":"PR #12 implements standalone stella_vslam Visual SLAM demo with Zenoh transport. Docker build + manual testing needed.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T19:25:26Z","event_type":"created","id":35,"issue_id":"turtlebot-maze-b2j","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T19:25:55Z","event_type":"status_changed","id":36,"issue_id":"turtlebot-maze-b2j","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-b2j\",\"title\":\"Test ArUco marker detection with camera\",\"description\":\"Verify that the ArUco markers (IDs 60 and 80) spawned in demo-world-enhanced are visible and detectable by the TurtleBot's camera. Drive the robot to marker locations and confirm detection using OpenCV's ArUco detector or a simple subscriber. This validates the markers placed by aruco_marker_spawner.launch.py are correctly positioned and oriented for camera-based pose estimation.\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-22T00:25:26Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-22T00:25:26Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T20:54:49Z","event_type":"closed","id":37,"issue_id":"turtlebot-maze-b2j","new_value":"ArUco marker detection verified working. DICT_4X4_100 detects marker ID 80 at 98% rate (53/54 frames). Fixed model SDFs to use flat plane PBR textures, corrected marker positions inside house bounds, updated test script.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T21:59:18Z","event_type":"created","id":38,"issue_id":"turtlebot-maze-jww","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T22:18:17Z","event_type":"created","id":39,"issue_id":"turtlebot-maze-3a5","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T22:18:23Z","event_type":"created","id":40,"issue_id":"turtlebot-maze-qob","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T22:18:25Z","event_type":"created","id":41,"issue_id":"turtlebot-maze-0tj","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T22:18:29Z","event_type":"created","id":42,"issue_id":"turtlebot-maze-j6n","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T22:18:33Z","event_type":"created","id":43,"issue_id":"turtlebot-maze-t84","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-21T22:18:37Z","event_type":"created","id":44,"issue_id":"turtlebot-maze-peq","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-26T19:16:59Z","event_type":"created","id":47,"issue_id":"turtlebot-maze-kzv","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-26T19:18:19Z","event_type":"status_changed","id":48,"issue_id":"turtlebot-maze-kzv","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-kzv\",\"title\":\"Fix duplicate Gazebo and RViz2 instances when opening project in VSCode remote container\",\"description\":\"When opening the project in a VSCode Remote Container (Dev Container), two instances of Gazebo and RViz2 launch instead of one.\\n\\n**Symptoms:**\\n- Two Gazebo windows appear simultaneously on session start\\n- Two RViz2 windows appear simultaneously on session start\\n- Likely causes resource contention and confusing state\\n\\n**Suspected causes:**\\n- Docker compose service(s) and the VSCode devcontainer postStartCommand / postCreateCommand may both be triggering the launch\\n- The devcontainer.json may have a launch command that conflicts with an auto-starting docker compose service\\n- A ROS 2 launch file may be sourced twice (e.g., via .bashrc + explicit entrypoint)\\n- Two devcontainer configurations or compose overrides both starting the simulation\\n\\n**Investigation steps:**\\n1. Check devcontainer.json for postStartCommand or postCreateCommand that launches Gazebo/RViz2\\n2. Check docker-compose.yml for services set to restart or autostart that also run the simulation\\n3. Check entrypoint scripts for duplicate launch calls\\n4. Check if VSCode opens the container AND runs docker compose up simultaneously\\n\\n**Expected behavior:**\\n- Exactly one Gazebo instance and one RViz2 instance launch when the project is opened in VSCode Remote Container\",\"status\":\"open\",\"priority\":1,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-27T00:16:59Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-27T00:16:59Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-26T19:19:35Z","event_type":"closed","id":49,"issue_id":"turtlebot-maze-kzv","new_value":"Fixed: added runServices: [dev] to devcontainer.json. Root cause was VSCode starting all compose services (including both demo-world and demo-world-enhanced) by default, each launching Gazebo and RViz2.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-27T07:34:21Z","event_type":"created","id":50,"issue_id":"turtlebot-maze-szq","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-27T07:35:57Z","event_type":"created","id":51,"issue_id":"turtlebot-maze-es3","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-27T07:36:39Z","event_type":"created","id":52,"issue_id":"turtlebot-maze-2b7","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-27T07:42:32Z","event_type":"status_changed","id":53,"issue_id":"turtlebot-maze-szq","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-szq\",\"title\":\"Fix slam_bridge PNG race condition: pass frames via Zenoh, not temp files\",\"description\":\"slam_bridge.py writes camera frames as PNG files to a temp directory and run_slam polls that directory for new files. run_slam reads files before the write is complete, causing 'libpng error: Read Error' on every frame. As a result feed_monocular_frame() always returns null and tb/slam/pose is never published.\\n\\n**Root Cause:**\\nThe file-based IPC between slam_bridge.py and run_slam has no write-completion guarantee. The PNG write is not atomic and run_slam picks up partial files.\\n\\n**Fix:**\\nEliminate the temp-file transport entirely. run_slam should receive raw image bytes directly — either via:\\n- A Zenoh subscriber inside run_slam (subscribe to the same image key slam_bridge subscribes to), OR\\n- A shared memory / pipe mechanism that provides write-completion semantics\\n\\nThe frame directory flag (-d) in run_slam was designed for file-based input, but since we already have Zenoh in the pipeline, run_slam should subscribe to Zenoh directly and process frames in the callback without any intermediate file I/O.\\n\\n**Impact:** tb/slam/pose is never published, making the entire Visual SLAM pipeline non-functional.\\n\\n**Affected files:**\\n- slam/slam_bridge.py — writes PNG files, passes -d flag to run_slam\\n- slam/run_slam.cc — polls frame directory, reads PNG files with cv::imread\",\"status\":\"open\",\"priority\":1,\"issue_type\":\"bug\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-27T12:34:21Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-27T12:34:21Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-27T07:46:05Z","event_type":"closed","id":54,"issue_id":"turtlebot-maze-szq","new_value":"Closed","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-27T10:11:52Z","event_type":"updated","id":55,"issue_id":"turtlebot-maze-es3","new_value":"{\"description\":\"Add Zenoh-native message recording to the project using zenoh-plugin-storage-manager AND connect the live SLAM data stream to the team Foxglove dashboard at https://app.foxglove.dev/ai-for-robotics/dashboard for real-time visualization of VSLAM pose, trajectory, and features.\\n\\n## Architecture\\n\\n```\\nROS 2 Nodes (nav2, Gazebo, behavior tree)\\n       ↓ DDS\\nzenoh-bridge-ros2dds  →  Zenoh key expressions (rt/camera/*, rt/odom, ...)\\n       ↓                         ↓\\nzenohd + storage-manager    foxglove-bridge service (NEW)\\n       ↓                         ↓\\nRocksDB backend           Foxglove WebSocket (ws://localhost:8765)\\n       ↓                         ↓\\nQuery API (replay)        https://app.foxglove.dev/ai-for-robotics/dashboard\\n```\\n\\n## Part A — Foxglove Live Visualization (Priority)\\n\\n### How to connect SLAM pose/trajectory/features to Foxglove\\n\\n**Option 1 — foxglove-bridge in ROS 2 container (recommended)**\\n\\nfoxglove-bridge is a ROS 2 node that auto-subscribes all topics and serves\\nthem over WebSocket on port 8765:\\n\\n```bash\\n# In demo-world-enhanced container\\nros2 run foxglove_bridge foxglove_bridge\\n```\\n\\nAdd a `foxglove-bridge` service to docker-compose.yaml:\\n```yaml\\nfoxglove-bridge:\\n  extends: overlay\\n  ports:\\n    - \\\"8765:8765\\\"\\n  command: ros2 run foxglove_bridge foxglove_bridge\\n```\\n\\nTopics automatically visualizable in Foxglove 3D panel:\\n- `/amcl_pose` → PoseWithCovariance (Nav2 localization)\\n- `/map` → OccupancyGrid (maze map)\\n- `/scan` → LaserScan (LIDAR)\\n- `/camera/image_raw` → Image\\n- `/tf`, `/tf_static` → coordinate frames\\n\\nFor tb/slam/pose (Zenoh, not ROS), a bridge service is needed (see Option 2).\\n\\n**Option 2 — foxglove-sdk publisher in slam_bridge.py (for tb/slam/pose)**\\n\\nThe SLAM pose (tb/slam/pose JSON) is Zenoh-only — not in ROS 2 DDS.\\nA foxglove-sdk server in slam_bridge.py publishes it directly to Foxglove:\\n\\n```python\\n# Install: pip install foxglove-sdk\\nimport foxglove\\nfrom foxglove.schemas import PoseInFrame, Pose, Vector3, Quaternion, Timestamp\\n\\nserver = foxglove.start_server(host=\\\"0.0.0.0\\\", port=8766)\\n\\n# In _read_poses(), after parsing JSON pose matrix:\\nimport json, math\\ndata = json.loads(line)\\nmat = data[\\\"pose\\\"]  # 12 elements of 4x4 matrix (row-major)\\nfoxglove.log(\\\"/slam/pose\\\", PoseInFrame(\\n    timestamp=Timestamp.now(),\\n    frame_id=\\\"odom\\\",\\n    pose=Pose(\\n        position=Vector3(x=mat[3], y=mat[7], z=mat[11]),\\n        orientation=rotation_matrix_to_quaternion(mat),\\n    )\\n))\\n```\\n\\nFor the trajectory (accumulated poses), use SceneUpdate with LinePrimitive,\\nor publish each pose as PoseInFrame and use the Foxglove \\\"Poses\\\" panel.\\n\\nConnect Foxglove to ws://localhost:8766 to see SLAM data.\\nConnect Foxglove to ws://localhost:8765 to see all ROS 2 topics.\\n\\n**Connecting to app.foxglove.dev:**\\n\\nThe team dashboard at https://app.foxglove.dev/ai-for-robotics/dashboard\\ncan connect live when the robot host is reachable:\\n1. Open the Foxglove app\\n2. Click \\\"Open connection\\\" → \\\"Foxglove WebSocket\\\"\\n3. Enter ws://\\u003cHOST_IP\\u003e:8765 (ROS topics) or ws://\\u003cHOST_IP\\u003e:8766 (SLAM)\\n4. Save the layout to the team dashboard\\n\\nFor cloud access (when host is behind NAT), use foxglove-relay or\\nngrok to tunnel port 8765/8766.\\n\\n**Panels to configure in the 3D panel:**\\n- `/map` → show OccupancyGrid as maze background\\n- `/amcl_pose` → robot position (Nav2)\\n- `/slam/pose` → SLAM estimated pose (different color)\\n- `/scan` → LIDAR point cloud ring\\n- `/camera/image_raw` → camera feed inset\\n- `/tf` → coordinate frame axes\\n\\n## Part B — Zenoh Storage / RocksDB Recording\\n\\n### Implementation Steps\\n\\n1. Configure zenoh-router container to load storage-manager plugin with RocksDB backend\\n2. Define storage key expressions:\\n   - `tb/**` — SLAM pose, status, ArUco detections\\n   - `intel_realsense_r200_depth/**` — camera and depth images\\n   - `rt/odom`, `rt/scan`, `rt/amcl_pose`, `rt/cmd_vel` — navigation telemetry\\n3. Mount a host volume into the zenoh-router container for persistent storage\\n4. Write a Python replay/export script using session.get() with _time=[start..end] selectors\\n5. Export to MCAP for offline Foxglove playback (mcap-python library)\\n\\n### Query API (replay)\\n\\n```python\\nsession = zenoh.open(conf)\\nresults = session.get(\\\"tb/**?_time=[2026-02-27T10:00:00..2026-02-27T10:10:00]\\\")\\nfor sample in results:\\n    print(sample.key_expr, sample.timestamp, len(bytes(sample.payload)), \\\"bytes\\\")\\n```\\n\\n### MCAP Export for offline Foxglove\\n\\n```python\\nfrom mcap.writer import Writer\\n# Write each stored sample as a channel/message in MCAP\\n# Foxglove opens MCAP files natively — drag-and-drop into app.foxglove.dev\\n```\\n\\n## References\\n\\n- foxglove-bridge: https://github.com/foxglove/ros-foxglove-bridge\\n- foxglove-sdk Python: https://docs.foxglove.dev/docs/sdk\\n- Foxglove PoseInFrame schema: https://docs.foxglove.dev/docs/visualization/message-schemas/pose-in-frame\\n- zenoh-plugin-storage-manager: https://github.com/eclipse-zenoh/zenoh-backend-rocksdb\\n- MCAP format: https://mcap.dev\"}","old_value":"{\"id\":\"turtlebot-maze-es3\",\"title\":\"Implement Zenoh storage for message traceability and dataset generation\",\"description\":\"Add Zenoh-native message recording to the project using zenoh-plugin-storage-manager. This enables traceability of all sensor/navigation messages and automated dataset generation without any ROS bag dependency.\\n\\n## Architecture\\n\\nAll sensor, SLAM, and navigation messages already flow through Zenoh (via zenoh-bridge-ros2dds). A storage plugin attached to the Zenoh router can record all of them automatically with zero changes to existing nodes.\\n\\n```\\nROS 2 Nodes (nav2, Gazebo, behavior tree)\\n       ↓ DDS\\nzenoh-bridge-ros2dds  →  Zenoh key expressions\\n       ↓\\nzenohd + storage-manager plugin\\n       ↓\\nRocksDB / InfluxDB / Filesystem backend\\n       ↓\\nQuery API (session.get() with time ranges)\\n```\\n\\n## Key Design Decisions\\n\\n- **Backend**: RocksDB for local edge recording (fast, persistent, no external service)\\n- **Scope**: Subscribe to `tb/**` (SLAM pose, status) and bridged ROS keys (camera, odom, scan, cmd_vel, amcl_pose)\\n- **Timestamps**: Use Zenoh's Hybrid Logical Clock (HLC) — nanosecond precision, no NTP required\\n- **No file transport**: All payloads stay in Zenoh — this also fixes the slam_bridge PNG race condition (turtlebot-maze-szq)\\n\\n## Implementation Steps\\n\\n1. Configure zenoh-router container to load storage-manager plugin with RocksDB backend\\n2. Define storage key expressions:\\n   - `tb/**` — SLAM pose, status, ArUco detections\\n   - `intel_realsense_r200_depth/**` — camera and depth images\\n   - `odom`, `scan`, `amcl_pose`, `cmd_vel` — navigation telemetry\\n3. Mount a host volume into the zenoh-router container for persistent storage\\n4. Write a Python replay/export script using session.get() with _time=[start..end] selectors\\n5. Document the dataset format (key, CDR payload bytes, HLC timestamp)\\n\\n## Query API (replay)\\n\\n```python\\nsession = zenoh.open(conf)\\nresults = session.get('tb/**?_time=[2026-02-27T10:00:00..2026-02-27T10:10:00]')\\nfor sample in results:\\n    print(sample.key_expr, sample.timestamp, len(bytes(sample.payload)), 'bytes')\\n```\\n\\n## References\\n\\n- zenoh-plugin-storage-manager: https://github.com/eclipse-zenoh/zenoh-backend-rocksdb\\n- zenoh-backend-influxdb (alternative for time-series analytics)\\n- Zenoh Selector time range syntax: `_time=[t1..t2]`\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"feature\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-27T12:35:57Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-27T12:35:57Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-28T19:16:40Z","event_type":"created","id":56,"issue_id":"turtlebot-maze-m3n","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-28T19:19:02Z","event_type":"status_changed","id":57,"issue_id":"turtlebot-maze-es3","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-es3\",\"title\":\"Implement Zenoh storage for message traceability and dataset generation\",\"description\":\"Add Zenoh-native message recording to the project using zenoh-plugin-storage-manager AND connect the live SLAM data stream to the team Foxglove dashboard at https://app.foxglove.dev/ai-for-robotics/dashboard for real-time visualization of VSLAM pose, trajectory, and features.\\n\\n## Architecture\\n\\n```\\nROS 2 Nodes (nav2, Gazebo, behavior tree)\\n       ↓ DDS\\nzenoh-bridge-ros2dds  →  Zenoh key expressions (rt/camera/*, rt/odom, ...)\\n       ↓                         ↓\\nzenohd + storage-manager    foxglove-bridge service (NEW)\\n       ↓                         ↓\\nRocksDB backend           Foxglove WebSocket (ws://localhost:8765)\\n       ↓                         ↓\\nQuery API (replay)        https://app.foxglove.dev/ai-for-robotics/dashboard\\n```\\n\\n## Part A — Foxglove Live Visualization (Priority)\\n\\n### How to connect SLAM pose/trajectory/features to Foxglove\\n\\n**Option 1 — foxglove-bridge in ROS 2 container (recommended)**\\n\\nfoxglove-bridge is a ROS 2 node that auto-subscribes all topics and serves\\nthem over WebSocket on port 8765:\\n\\n```bash\\n# In demo-world-enhanced container\\nros2 run foxglove_bridge foxglove_bridge\\n```\\n\\nAdd a `foxglove-bridge` service to docker-compose.yaml:\\n```yaml\\nfoxglove-bridge:\\n  extends: overlay\\n  ports:\\n    - \\\"8765:8765\\\"\\n  command: ros2 run foxglove_bridge foxglove_bridge\\n```\\n\\nTopics automatically visualizable in Foxglove 3D panel:\\n- `/amcl_pose` → PoseWithCovariance (Nav2 localization)\\n- `/map` → OccupancyGrid (maze map)\\n- `/scan` → LaserScan (LIDAR)\\n- `/camera/image_raw` → Image\\n- `/tf`, `/tf_static` → coordinate frames\\n\\nFor tb/slam/pose (Zenoh, not ROS), a bridge service is needed (see Option 2).\\n\\n**Option 2 — foxglove-sdk publisher in slam_bridge.py (for tb/slam/pose)**\\n\\nThe SLAM pose (tb/slam/pose JSON) is Zenoh-only — not in ROS 2 DDS.\\nA foxglove-sdk server in slam_bridge.py publishes it directly to Foxglove:\\n\\n```python\\n# Install: pip install foxglove-sdk\\nimport foxglove\\nfrom foxglove.schemas import PoseInFrame, Pose, Vector3, Quaternion, Timestamp\\n\\nserver = foxglove.start_server(host=\\\"0.0.0.0\\\", port=8766)\\n\\n# In _read_poses(), after parsing JSON pose matrix:\\nimport json, math\\ndata = json.loads(line)\\nmat = data[\\\"pose\\\"]  # 12 elements of 4x4 matrix (row-major)\\nfoxglove.log(\\\"/slam/pose\\\", PoseInFrame(\\n    timestamp=Timestamp.now(),\\n    frame_id=\\\"odom\\\",\\n    pose=Pose(\\n        position=Vector3(x=mat[3], y=mat[7], z=mat[11]),\\n        orientation=rotation_matrix_to_quaternion(mat),\\n    )\\n))\\n```\\n\\nFor the trajectory (accumulated poses), use SceneUpdate with LinePrimitive,\\nor publish each pose as PoseInFrame and use the Foxglove \\\"Poses\\\" panel.\\n\\nConnect Foxglove to ws://localhost:8766 to see SLAM data.\\nConnect Foxglove to ws://localhost:8765 to see all ROS 2 topics.\\n\\n**Connecting to app.foxglove.dev:**\\n\\nThe team dashboard at https://app.foxglove.dev/ai-for-robotics/dashboard\\ncan connect live when the robot host is reachable:\\n1. Open the Foxglove app\\n2. Click \\\"Open connection\\\" → \\\"Foxglove WebSocket\\\"\\n3. Enter ws://\\u003cHOST_IP\\u003e:8765 (ROS topics) or ws://\\u003cHOST_IP\\u003e:8766 (SLAM)\\n4. Save the layout to the team dashboard\\n\\nFor cloud access (when host is behind NAT), use foxglove-relay or\\nngrok to tunnel port 8765/8766.\\n\\n**Panels to configure in the 3D panel:**\\n- `/map` → show OccupancyGrid as maze background\\n- `/amcl_pose` → robot position (Nav2)\\n- `/slam/pose` → SLAM estimated pose (different color)\\n- `/scan` → LIDAR point cloud ring\\n- `/camera/image_raw` → camera feed inset\\n- `/tf` → coordinate frame axes\\n\\n## Part B — Zenoh Storage / RocksDB Recording\\n\\n### Implementation Steps\\n\\n1. Configure zenoh-router container to load storage-manager plugin with RocksDB backend\\n2. Define storage key expressions:\\n   - `tb/**` — SLAM pose, status, ArUco detections\\n   - `intel_realsense_r200_depth/**` — camera and depth images\\n   - `rt/odom`, `rt/scan`, `rt/amcl_pose`, `rt/cmd_vel` — navigation telemetry\\n3. Mount a host volume into the zenoh-router container for persistent storage\\n4. Write a Python replay/export script using session.get() with _time=[start..end] selectors\\n5. Export to MCAP for offline Foxglove playback (mcap-python library)\\n\\n### Query API (replay)\\n\\n```python\\nsession = zenoh.open(conf)\\nresults = session.get(\\\"tb/**?_time=[2026-02-27T10:00:00..2026-02-27T10:10:00]\\\")\\nfor sample in results:\\n    print(sample.key_expr, sample.timestamp, len(bytes(sample.payload)), \\\"bytes\\\")\\n```\\n\\n### MCAP Export for offline Foxglove\\n\\n```python\\nfrom mcap.writer import Writer\\n# Write each stored sample as a channel/message in MCAP\\n# Foxglove opens MCAP files natively — drag-and-drop into app.foxglove.dev\\n```\\n\\n## References\\n\\n- foxglove-bridge: https://github.com/foxglove/ros-foxglove-bridge\\n- foxglove-sdk Python: https://docs.foxglove.dev/docs/sdk\\n- Foxglove PoseInFrame schema: https://docs.foxglove.dev/docs/visualization/message-schemas/pose-in-frame\\n- zenoh-plugin-storage-manager: https://github.com/eclipse-zenoh/zenoh-backend-rocksdb\\n- MCAP format: https://mcap.dev\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"feature\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-02-27T12:35:57Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-02-27T15:11:53Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-02-28T19:28:09Z","event_type":"closed","id":58,"issue_id":"turtlebot-maze-es3","new_value":"Part A (foxglove-bridge) already done. Part B: zenoh-storage.json5 (in-memory storage-manager), detection_logger.py (persistent JSONL), query_detections.py (query/export/CSV/summary). SLAM storage split into turtlebot-maze-m3n.","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T08:50:48Z","event_type":"created","id":59,"issue_id":"turtlebot-maze-2mc","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T08:51:06Z","event_type":"created","id":60,"issue_id":"turtlebot-maze-3ml","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T08:51:22Z","event_type":"created","id":61,"issue_id":"turtlebot-maze-dgm","new_value":"","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T08:58:05Z","event_type":"status_changed","id":62,"issue_id":"turtlebot-maze-3ml","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-3ml\",\"title\":\"Audit R200 usage in Nav2 and assess D435i migration impact\",\"description\":\"## Context\\n\\nBefore swapping the physical camera from R200 to D435i, we need to understand all the places the R200 is referenced in the navigation stack and assess what changes are needed to keep Nav2 functional.\\n\\n## Investigation Tasks\\n\\n### 1. Find all R200 references in the repo\\n- Search for 'r200', 'realsense_r200', 'intel_realsense' in all config files, launch files, URDF/SDF models, bridge configs\\n- Key files to check:\\n  - tb_worlds/configs/turtlebot3_bridge.yaml (DDS↔Zenoh bridge topic mapping)\\n  - tb_worlds/urdf/ (robot model, camera mount, TF frames)\\n  - tb_worlds/maps/ (any sensor-specific map params)\\n  - tb_autonomy/config/ (Nav2 params: costmap, sensor sources)\\n\\n### 2. Nav2 sensor sources\\n- Identify which Nav2 costmap plugins consume depth or pointcloud data from the R200\\n- Check obstacle_layer and voxel_layer sensor_sources in nav2_params.yaml\\n- Determine if Nav2 uses the depth stream for obstacle avoidance or only the RGB for vision behaviors\\n\\n### 3. TF frame changes\\n- R200 publishes TF frame: camera_link, camera_depth_frame, camera_color_frame\\n- D435i publishes: camera_link, camera_depth_frame, camera_color_frame, camera_imu_frame\\n- Assess if TF frame names differ between R200 and D435i ROS 2 driver (realsense2_camera)\\n- Check if URDF/xacro hardcodes R200-specific frame names\\n\\n### 4. ROS topic name changes\\n- R200 (librealsense legacy): /camera/rgb/image_raw, /camera/depth/image_raw\\n- D435i (realsense2_camera): /camera/color/image_raw, /camera/depth/image_rect_raw, /camera/aligned_depth_to_color/image_raw\\n- List all topic subscriptions in tb_autonomy that would need updating\\n\\n### 5. Simulation model\\n- The Gazebo world currently simulates an R200-compatible camera\\n- Assess whether tb_worlds needs a new SDF camera plugin or xacro model to simulate D435i topics\\n- Check if the simulation resolution (320×240) is still appropriate or should be increased\\n\\n## Deliverables\\n- Written summary of all impacted files with specific line numbers\\n- List of Nav2 param changes needed\\n- List of TF frame / topic remapping changes needed\\n- Assessment: is the swap a simple topic rename or does it require URDF/SDF model changes?\\n- Update this issue with findings before closing\",\"status\":\"open\",\"priority\":1,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-03-01T13:51:06Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-03-01T13:51:06Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T08:59:17Z","event_type":"updated","id":63,"issue_id":"turtlebot-maze-3ml","new_value":"{\"description\":\"## Context\\n\\nBefore swapping the physical camera from R200 to D435i, we need to understand all the places the R200 is referenced in the navigation stack and assess what changes are needed to keep Nav2 functional.\\n\\n---\\n\\n## FINDINGS (completed 2026-03-01)\\n\\n### TL;DR: Nav2 does NOT use the camera. The D435i swap is safe for navigation.\\n\\n---\\n\\n### 1. Nav2 sensor sources — camera-free\\n\\n`tb_worlds/configs/nav2_params.yaml` uses only LiDAR for all costmap layers:\\n\\n```yaml\\nlocal_costmap:\\n  voxel_layer:\\n    observation_sources: scan   # LaserScan only — no camera\\n    scan:\\n      topic: /scan\\n      data_type: \\\"LaserScan\\\"\\n\\nglobal_costmap:\\n  obstacle_layer:\\n    observation_sources: scan   # LaserScan only — no camera\\n\\ncollision_monitor:\\n  observation_sources: [\\\"scan\\\"] # LaserScan only\\n```\\n\\nAMCL uses scan (LiDAR) for localisation. No voxel layer, no depth camera, no pointcloud source anywhere in nav2_params.yaml. **Zero Nav2 parameter changes are needed for the D435i swap.**\\n\\n---\\n\\n### 2. R200 references — complete list\\n\\n| File | Line(s) | Type | Impact on D435i swap |\\n|------|---------|------|----------------------|\\n| `tb_worlds/urdf/gz_waffle.sdf.xacro` | 392, 415 | SDF sensor name + topic | Must rename |\\n| `tb_worlds/configs/turtlebot3_bridge.yaml` | 53–66 | Gz→ROS bridge topic names | Must rename |\\n| `tb_worlds/configs/turtlebot3_bridge.yaml` | 72–76 | Legacy `/camera/image_raw` alias | Keep — vision behaviors depend on it |\\n| `tb_autonomy/src/vision_behaviors.cpp` | 27 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\\n| `tb_autonomy/python/tb_behaviors/vision.py` | 88 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\\n| `tb_autonomy/scripts/test_vision.py` | 31 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\\n| `slam/slam_bridge.py` | 262, 268 | `--image-key` / `--depth-key` defaults | Handled by turtlebot-maze-2mc |\\n| `slam/test_aruco_detection.py` | 53 | `--image-key` default | Update alongside 2mc |\\n| `docker-compose.yaml` | 239–240 | demo-slam command args | Handled by turtlebot-maze-2mc |\\n| `docs/plans/` | various | Plan text / code examples | Docs-only, no runtime impact |\\n\\n---\\n\\n### 3. TF frames\\n\\nThe SDF model defines (gz_waffle.sdf.xacro):\\n- `camera_link` (line 368) — parent link for the camera assembly\\n- `camera_depth_frame` (line 396, via `\\u003cgz_frame_id\\u003e`) — sensor frame\\n\\nThe real D435i via `realsense2_camera` driver publishes:\\n`camera_link`, `camera_depth_frame`, `camera_depth_optical_frame`,\\n`camera_color_frame`, `camera_color_optical_frame`, `camera_imu_frame`\\n\\n**Frame names match** — `camera_link` and `camera_depth_frame` are identical between the simulated R200 and the real D435i driver. No TF remapping is needed. The additional frames the D435i publishes (optical frames, IMU) are additive and do not break anything.\\n\\n---\\n\\n### 4. ROS topic changes needed\\n\\n| Current (R200 / simulation) | D435i (realsense2_camera) | Users |\\n|----------------------------|--------------------------|-------|\\n| `/intel_realsense_r200_depth/image_raw` | `/camera/color/image_raw` | SLAM (2mc), detector (dgm) |\\n| `/intel_realsense_r200_depth/depth/image_raw` | `/camera/depth/image_rect_raw` | SLAM (2mc) |\\n| `/intel_realsense_r200_depth/points` | `/camera/depth/color/points` | Nothing currently — unused |\\n| `/camera/image_raw` (alias) | keep as remap or use `/camera/color/image_raw` | vision_behaviors.cpp, vision.py |\\n\\nThe vision behaviors subscribe to `/camera/image_raw`. With the real D435i the simplest fix is to add a ROS 2 topic remap in the launch file (`/camera/color/image_raw -\\u003e /camera/image_raw`) rather than changing all subscriber code.\\n\\n---\\n\\n### 5. Simulation model changes required\\n\\nTo simulate a D435i in Gazebo (cosmetic but good hygiene):\\n\\n**gz_waffle.sdf.xacro** — rename sensor:\\n```xml\\n\\u003c!-- Before --\\u003e\\n\\u003csensor name=\\\"intel_realsense_r200_depth\\\" type=\\\"rgbd_camera\\\"\\u003e\\n  \\u003ctopic\\u003eintel_realsense_r200_depth\\u003c/topic\\u003e\\n\\n\\u003c!-- After --\\u003e\\n\\u003csensor name=\\\"camera_realsense_d435i\\\" type=\\\"rgbd_camera\\\"\\u003e\\n  \\u003ctopic\\u003ecamera/realsense_d435i\\u003c/topic\\u003e\\n```\\n\\n**turtlebot3_bridge.yaml** — update Gz topic names to match:\\n```yaml\\n- ros_topic_name: \\\"/camera/color/image_raw\\\"\\n  gz_topic_name: \\\"/camera/realsense_d435i/image\\\"\\n- ros_topic_name: \\\"/camera/depth/image_rect_raw\\\"\\n  gz_topic_name: \\\"/camera/realsense_d435i/depth_image\\\"\\n- ros_topic_name: \\\"/camera/depth/color/points\\\"\\n  gz_topic_name: \\\"/camera/realsense_d435i/points\\\"\\n# Keep legacy alias for vision behaviors:\\n- ros_topic_name: \\\"/camera/image_raw\\\"\\n  gz_topic_name: \\\"/camera/realsense_d435i/image\\\"\\n```\\n\\nResolution can stay at 320×240 for simulation (fast, already tuned).\\n\\n---\\n\\n### 6. Impact assessment summary\\n\\n| Layer | Impact | Changes needed |\\n|-------|--------|----------------|\\n| Nav2 (AMCL, costmaps, planners) | **None** | None — LiDAR only |\\n| TF frames | **None** | frame names identical |\\n| Vision behaviors (HSV/YOLO color detection) | **Minimal** | Add launch file remap `/camera/color/image_raw → /camera/image_raw` |\\n| SLAM pipeline | Medium | Handled by turtlebot-maze-2mc |\\n| Object detector (Zenoh) | Minimal | Handled by turtlebot-maze-dgm |\\n| Simulation model | Cosmetic | Rename SDF sensor + bridge topics |\\n\\n---\\n\\n## Required Changes (scoped to this issue)\\n\\n1. **`tb_worlds/urdf/gz_waffle.sdf.xacro`** — rename sensor from `intel_realsense_r200_depth` to `camera_realsense_d435i`, update `\\u003ctopic\\u003e`\\n2. **`tb_worlds/configs/turtlebot3_bridge.yaml`** — update Gz→ROS topic bridges to D435i names; keep `/camera/image_raw` legacy alias\\n3. **Launch file remap** — add `/camera/color/image_raw → /camera/image_raw` remap for real-robot deployment so vision behaviors need no code change\\n\\n## Investigation Tasks\\n\\n### 1. Find all R200 references in the repo ✅\\n### 2. Nav2 sensor sources ✅\\n### 3. TF frame changes ✅\\n### 4. ROS topic name changes ✅\\n### 5. Simulation model ✅\\n\\n## Deliverables\\n- Written summary of all impacted files with specific line numbers ✅\\n- List of Nav2 param changes needed ✅ (none)\\n- List of TF frame / topic remapping changes needed ✅\\n- Assessment: is the swap a simple topic rename or does it require URDF/SDF model changes? ✅ Primarily topic rename + SDF sensor rename; TF unchanged; Nav2 unaffected\"}","old_value":"{\"id\":\"turtlebot-maze-3ml\",\"title\":\"Audit R200 usage in Nav2 and assess D435i migration impact\",\"description\":\"## Context\\n\\nBefore swapping the physical camera from R200 to D435i, we need to understand all the places the R200 is referenced in the navigation stack and assess what changes are needed to keep Nav2 functional.\\n\\n## Investigation Tasks\\n\\n### 1. Find all R200 references in the repo\\n- Search for 'r200', 'realsense_r200', 'intel_realsense' in all config files, launch files, URDF/SDF models, bridge configs\\n- Key files to check:\\n  - tb_worlds/configs/turtlebot3_bridge.yaml (DDS↔Zenoh bridge topic mapping)\\n  - tb_worlds/urdf/ (robot model, camera mount, TF frames)\\n  - tb_worlds/maps/ (any sensor-specific map params)\\n  - tb_autonomy/config/ (Nav2 params: costmap, sensor sources)\\n\\n### 2. Nav2 sensor sources\\n- Identify which Nav2 costmap plugins consume depth or pointcloud data from the R200\\n- Check obstacle_layer and voxel_layer sensor_sources in nav2_params.yaml\\n- Determine if Nav2 uses the depth stream for obstacle avoidance or only the RGB for vision behaviors\\n\\n### 3. TF frame changes\\n- R200 publishes TF frame: camera_link, camera_depth_frame, camera_color_frame\\n- D435i publishes: camera_link, camera_depth_frame, camera_color_frame, camera_imu_frame\\n- Assess if TF frame names differ between R200 and D435i ROS 2 driver (realsense2_camera)\\n- Check if URDF/xacro hardcodes R200-specific frame names\\n\\n### 4. ROS topic name changes\\n- R200 (librealsense legacy): /camera/rgb/image_raw, /camera/depth/image_raw\\n- D435i (realsense2_camera): /camera/color/image_raw, /camera/depth/image_rect_raw, /camera/aligned_depth_to_color/image_raw\\n- List all topic subscriptions in tb_autonomy that would need updating\\n\\n### 5. Simulation model\\n- The Gazebo world currently simulates an R200-compatible camera\\n- Assess whether tb_worlds needs a new SDF camera plugin or xacro model to simulate D435i topics\\n- Check if the simulation resolution (320×240) is still appropriate or should be increased\\n\\n## Deliverables\\n- Written summary of all impacted files with specific line numbers\\n- List of Nav2 param changes needed\\n- List of TF frame / topic remapping changes needed\\n- Assessment: is the swap a simple topic rename or does it require URDF/SDF model changes?\\n- Update this issue with findings before closing\",\"status\":\"in_progress\",\"priority\":1,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-03-01T13:51:06Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-03-01T13:58:05Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T09:00:07Z","event_type":"status_changed","id":64,"issue_id":"turtlebot-maze-3ml","new_value":"{\"status\":\"completed\"}","old_value":"{\"id\":\"turtlebot-maze-3ml\",\"title\":\"Audit R200 usage in Nav2 and assess D435i migration impact\",\"description\":\"## Context\\n\\nBefore swapping the physical camera from R200 to D435i, we need to understand all the places the R200 is referenced in the navigation stack and assess what changes are needed to keep Nav2 functional.\\n\\n---\\n\\n## FINDINGS (completed 2026-03-01)\\n\\n### TL;DR: Nav2 does NOT use the camera. The D435i swap is safe for navigation.\\n\\n---\\n\\n### 1. Nav2 sensor sources — camera-free\\n\\n`tb_worlds/configs/nav2_params.yaml` uses only LiDAR for all costmap layers:\\n\\n```yaml\\nlocal_costmap:\\n  voxel_layer:\\n    observation_sources: scan   # LaserScan only — no camera\\n    scan:\\n      topic: /scan\\n      data_type: \\\"LaserScan\\\"\\n\\nglobal_costmap:\\n  obstacle_layer:\\n    observation_sources: scan   # LaserScan only — no camera\\n\\ncollision_monitor:\\n  observation_sources: [\\\"scan\\\"] # LaserScan only\\n```\\n\\nAMCL uses scan (LiDAR) for localisation. No voxel layer, no depth camera, no pointcloud source anywhere in nav2_params.yaml. **Zero Nav2 parameter changes are needed for the D435i swap.**\\n\\n---\\n\\n### 2. R200 references — complete list\\n\\n| File | Line(s) | Type | Impact on D435i swap |\\n|------|---------|------|----------------------|\\n| `tb_worlds/urdf/gz_waffle.sdf.xacro` | 392, 415 | SDF sensor name + topic | Must rename |\\n| `tb_worlds/configs/turtlebot3_bridge.yaml` | 53–66 | Gz→ROS bridge topic names | Must rename |\\n| `tb_worlds/configs/turtlebot3_bridge.yaml` | 72–76 | Legacy `/camera/image_raw` alias | Keep — vision behaviors depend on it |\\n| `tb_autonomy/src/vision_behaviors.cpp` | 27 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\\n| `tb_autonomy/python/tb_behaviors/vision.py` | 88 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\\n| `tb_autonomy/scripts/test_vision.py` | 31 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\\n| `slam/slam_bridge.py` | 262, 268 | `--image-key` / `--depth-key` defaults | Handled by turtlebot-maze-2mc |\\n| `slam/test_aruco_detection.py` | 53 | `--image-key` default | Update alongside 2mc |\\n| `docker-compose.yaml` | 239–240 | demo-slam command args | Handled by turtlebot-maze-2mc |\\n| `docs/plans/` | various | Plan text / code examples | Docs-only, no runtime impact |\\n\\n---\\n\\n### 3. TF frames\\n\\nThe SDF model defines (gz_waffle.sdf.xacro):\\n- `camera_link` (line 368) — parent link for the camera assembly\\n- `camera_depth_frame` (line 396, via `\\u003cgz_frame_id\\u003e`) — sensor frame\\n\\nThe real D435i via `realsense2_camera` driver publishes:\\n`camera_link`, `camera_depth_frame`, `camera_depth_optical_frame`,\\n`camera_color_frame`, `camera_color_optical_frame`, `camera_imu_frame`\\n\\n**Frame names match** — `camera_link` and `camera_depth_frame` are identical between the simulated R200 and the real D435i driver. No TF remapping is needed. The additional frames the D435i publishes (optical frames, IMU) are additive and do not break anything.\\n\\n---\\n\\n### 4. ROS topic changes needed\\n\\n| Current (R200 / simulation) | D435i (realsense2_camera) | Users |\\n|----------------------------|--------------------------|-------|\\n| `/intel_realsense_r200_depth/image_raw` | `/camera/color/image_raw` | SLAM (2mc), detector (dgm) |\\n| `/intel_realsense_r200_depth/depth/image_raw` | `/camera/depth/image_rect_raw` | SLAM (2mc) |\\n| `/intel_realsense_r200_depth/points` | `/camera/depth/color/points` | Nothing currently — unused |\\n| `/camera/image_raw` (alias) | keep as remap or use `/camera/color/image_raw` | vision_behaviors.cpp, vision.py |\\n\\nThe vision behaviors subscribe to `/camera/image_raw`. With the real D435i the simplest fix is to add a ROS 2 topic remap in the launch file (`/camera/color/image_raw -\\u003e /camera/image_raw`) rather than changing all subscriber code.\\n\\n---\\n\\n### 5. Simulation model changes required\\n\\nTo simulate a D435i in Gazebo (cosmetic but good hygiene):\\n\\n**gz_waffle.sdf.xacro** — rename sensor:\\n```xml\\n\\u003c!-- Before --\\u003e\\n\\u003csensor name=\\\"intel_realsense_r200_depth\\\" type=\\\"rgbd_camera\\\"\\u003e\\n  \\u003ctopic\\u003eintel_realsense_r200_depth\\u003c/topic\\u003e\\n\\n\\u003c!-- After --\\u003e\\n\\u003csensor name=\\\"camera_realsense_d435i\\\" type=\\\"rgbd_camera\\\"\\u003e\\n  \\u003ctopic\\u003ecamera/realsense_d435i\\u003c/topic\\u003e\\n```\\n\\n**turtlebot3_bridge.yaml** — update Gz topic names to match:\\n```yaml\\n- ros_topic_name: \\\"/camera/color/image_raw\\\"\\n  gz_topic_name: \\\"/camera/realsense_d435i/image\\\"\\n- ros_topic_name: \\\"/camera/depth/image_rect_raw\\\"\\n  gz_topic_name: \\\"/camera/realsense_d435i/depth_image\\\"\\n- ros_topic_name: \\\"/camera/depth/color/points\\\"\\n  gz_topic_name: \\\"/camera/realsense_d435i/points\\\"\\n# Keep legacy alias for vision behaviors:\\n- ros_topic_name: \\\"/camera/image_raw\\\"\\n  gz_topic_name: \\\"/camera/realsense_d435i/image\\\"\\n```\\n\\nResolution can stay at 320×240 for simulation (fast, already tuned).\\n\\n---\\n\\n### 6. Impact assessment summary\\n\\n| Layer | Impact | Changes needed |\\n|-------|--------|----------------|\\n| Nav2 (AMCL, costmaps, planners) | **None** | None — LiDAR only |\\n| TF frames | **None** | frame names identical |\\n| Vision behaviors (HSV/YOLO color detection) | **Minimal** | Add launch file remap `/camera/color/image_raw → /camera/image_raw` |\\n| SLAM pipeline | Medium | Handled by turtlebot-maze-2mc |\\n| Object detector (Zenoh) | Minimal | Handled by turtlebot-maze-dgm |\\n| Simulation model | Cosmetic | Rename SDF sensor + bridge topics |\\n\\n---\\n\\n## Required Changes (scoped to this issue)\\n\\n1. **`tb_worlds/urdf/gz_waffle.sdf.xacro`** — rename sensor from `intel_realsense_r200_depth` to `camera_realsense_d435i`, update `\\u003ctopic\\u003e`\\n2. **`tb_worlds/configs/turtlebot3_bridge.yaml`** — update Gz→ROS topic bridges to D435i names; keep `/camera/image_raw` legacy alias\\n3. **Launch file remap** — add `/camera/color/image_raw → /camera/image_raw` remap for real-robot deployment so vision behaviors need no code change\\n\\n## Investigation Tasks\\n\\n### 1. Find all R200 references in the repo ✅\\n### 2. Nav2 sensor sources ✅\\n### 3. TF frame changes ✅\\n### 4. ROS topic name changes ✅\\n### 5. Simulation model ✅\\n\\n## Deliverables\\n- Written summary of all impacted files with specific line numbers ✅\\n- List of Nav2 param changes needed ✅ (none)\\n- List of TF frame / topic remapping changes needed ✅\\n- Assessment: is the swap a simple topic rename or does it require URDF/SDF model changes? ✅ Primarily topic rename + SDF sensor rename; TF unchanged; Nav2 unaffected\",\"status\":\"in_progress\",\"priority\":1,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-03-01T13:51:06Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-03-01T13:59:18Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T09:02:14Z","event_type":"status_changed","id":65,"issue_id":"turtlebot-maze-2mc","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-2mc\",\"title\":\"Swap R200 with D435i in SLAM pipeline\",\"description\":\"## Context\\n\\nThe Intel RealSense R200's RGB camera uses a rolling shutter, which causes geometric distortion during motion and is the root cause of rapid tracking loss in stella_vslam (observed: tracking lost within 5 frames of initialization). The D435i has a global shutter RGB camera and a built-in IMU, making it the appropriate sensor for visual SLAM.\\n\\nDocumented in: docs/plans/2026-02-21-stella-vslam-plan.md (Camera Model Assessment section)\\n\\n## Scope\\n\\n### 1. Zenoh key updates\\n- Simulation: image key is rt/intel_realsense_r200_depth/image_raw (from zenoh-bridge-ros2dds)\\n- Real robot (D435i): ROS 2 driver publishes camera/color/image_raw and camera/depth/image_rect_raw → bridged as rt/camera/color/image_raw and rt/camera/depth/image_rect_raw\\n- Update slam_bridge.py --image-key and --depth-key defaults\\n- Update docker-compose.yaml demo-slam command arguments\\n\\n### 2. Camera config\\n- Create slam/config/d435i_rgbd.yaml with real calibrated intrinsics\\n- Switch from monocular (feed_monocular_frame) to RGBD mode (feed_RGBD_frame) in run_slam.cc\\n- D435i color: fx≈615, fy≈615, cx≈320, cy≈240 at 640×480 (calibrate to confirm)\\n- Add depth_factor: 0.001 (D435i depth is in mm, stella_vslam expects metres)\\n- Set color_order: RGB (D435i publishes RGB, not BGR)\\n\\n### 3. run_slam.cc rewrite for RGBD\\n- Add depth image stdin channel (second stream after colour frame)\\n- Call slam-\\u003efeed_RGBD_frame(color, depth, timestamp) instead of feed_monocular_frame\\n- Wire format extension: after colour header+pixels, send depth header+pixels (uint16, millimetres)\\n\\n### 4. slam_bridge.py updates\\n- Subscribe to both image and depth Zenoh keys\\n- Deserialize depth as 16-bit single-channel image\\n- Send paired (colour, depth) frames via stdin pipe\\n- Consider IMU subscription for future VINS upgrade\\n\\n### 5. Simulation compatibility\\n- Gazebo SDF model in tb_worlds should publish a simulated depth stream on the D435i topic name\\n- Or: add a --simulate flag that uses the existing 320×240 colour-only stream with monocular mode\\n\\n## Acceptance Criteria\\n- stella_vslam initializes and holds tracking for \\u003e60 seconds in demo-world-enhanced\\n- Pose records appear in tb/slam/pose Zenoh topic (verified via slam-logger JSONL)\\n- slam/config/d435i_rgbd.yaml committed with documented calibration source\\n- run_slam.cc compiles cleanly and processes RGBD frames from D435i\",\"status\":\"open\",\"priority\":1,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-03-01T13:50:48Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-03-01T13:50:48Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T09:21:14Z","event_type":"updated","id":66,"issue_id":"turtlebot-maze-2mc","new_value":"{\"notes\":\"COMPLETED. Root causes found and fixed:\\n1. Zenoh key prefix: zenoh-bridge-ros2dds strips the DDS rt/ prefix when publishing to Zenoh. slam_bridge was subscribing to rt/camera/* but bridge publishes to camera/*. Fixed in slam_bridge.py defaults and docker-compose.yaml demo-slam command.\\n2. Volume mounts: turtlebot3_bridge.yaml and gz_waffle.sdf.xacro volume-mounted to demo-world-enhanced; ros_gz_bridge now bridges /camera/realsense_d435i/* -\\u003e /camera/color/image_raw and /camera/depth/image_rect_raw correctly.\\nVerification: demo-slam shows Mode=rgbd, Image key=camera/color/image_raw, Processed 50 frames.\"}","old_value":"{\"id\":\"turtlebot-maze-2mc\",\"title\":\"Swap R200 with D435i in SLAM pipeline\",\"description\":\"## Context\\n\\nThe Intel RealSense R200's RGB camera uses a rolling shutter, which causes geometric distortion during motion and is the root cause of rapid tracking loss in stella_vslam (observed: tracking lost within 5 frames of initialization). The D435i has a global shutter RGB camera and a built-in IMU, making it the appropriate sensor for visual SLAM.\\n\\nDocumented in: docs/plans/2026-02-21-stella-vslam-plan.md (Camera Model Assessment section)\\n\\n## Scope\\n\\n### 1. Zenoh key updates\\n- Simulation: image key is rt/intel_realsense_r200_depth/image_raw (from zenoh-bridge-ros2dds)\\n- Real robot (D435i): ROS 2 driver publishes camera/color/image_raw and camera/depth/image_rect_raw → bridged as rt/camera/color/image_raw and rt/camera/depth/image_rect_raw\\n- Update slam_bridge.py --image-key and --depth-key defaults\\n- Update docker-compose.yaml demo-slam command arguments\\n\\n### 2. Camera config\\n- Create slam/config/d435i_rgbd.yaml with real calibrated intrinsics\\n- Switch from monocular (feed_monocular_frame) to RGBD mode (feed_RGBD_frame) in run_slam.cc\\n- D435i color: fx≈615, fy≈615, cx≈320, cy≈240 at 640×480 (calibrate to confirm)\\n- Add depth_factor: 0.001 (D435i depth is in mm, stella_vslam expects metres)\\n- Set color_order: RGB (D435i publishes RGB, not BGR)\\n\\n### 3. run_slam.cc rewrite for RGBD\\n- Add depth image stdin channel (second stream after colour frame)\\n- Call slam-\\u003efeed_RGBD_frame(color, depth, timestamp) instead of feed_monocular_frame\\n- Wire format extension: after colour header+pixels, send depth header+pixels (uint16, millimetres)\\n\\n### 4. slam_bridge.py updates\\n- Subscribe to both image and depth Zenoh keys\\n- Deserialize depth as 16-bit single-channel image\\n- Send paired (colour, depth) frames via stdin pipe\\n- Consider IMU subscription for future VINS upgrade\\n\\n### 5. Simulation compatibility\\n- Gazebo SDF model in tb_worlds should publish a simulated depth stream on the D435i topic name\\n- Or: add a --simulate flag that uses the existing 320×240 colour-only stream with monocular mode\\n\\n## Acceptance Criteria\\n- stella_vslam initializes and holds tracking for \\u003e60 seconds in demo-world-enhanced\\n- Pose records appear in tb/slam/pose Zenoh topic (verified via slam-logger JSONL)\\n- slam/config/d435i_rgbd.yaml committed with documented calibration source\\n- run_slam.cc compiles cleanly and processes RGBD frames from D435i\",\"status\":\"in_progress\",\"priority\":1,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-03-01T13:50:48Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-03-01T14:02:15Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T09:21:16Z","event_type":"closed","id":67,"issue_id":"turtlebot-maze-2mc","new_value":"Closed","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T09:26:52Z","event_type":"status_changed","id":68,"issue_id":"turtlebot-maze-dgm","new_value":"{\"status\":\"in_progress\"}","old_value":"{\"id\":\"turtlebot-maze-dgm\",\"title\":\"Validate D435i for YOLOv8 object detection\",\"description\":\"## Context\\n\\nOnce the R200→D435i camera swap is complete for SLAM (turtlebot-maze-2mc) and the Nav2 impact is resolved (turtlebot-maze-3ml), the YOLOv8 object detection pipeline also needs to be validated with the D435i.\\n\\nThe current object_detector.py subscribes to the camera image Zenoh key. The D435i publishes on different topic names than the R200, which propagate through zenoh-bridge-ros2dds to different Zenoh keys.\\n\\n## Tasks\\n\\n### 1. Update Zenoh image key\\n- Current default in object_detector.py: --image-key camera/image_raw (or rt/intel_realsense_r200_depth/image_raw)\\n- D435i via realsense2_camera + zenoh-bridge: rt/camera/color/image_raw\\n- Update object_detector.py --image-key default and docker-compose.yaml detector command\\n\\n### 2. Verify image encoding compatibility\\n- R200 RGB: publishes rgb8 or bgr8\\n- D435i color: publishes rgb8 by default from realsense2_camera driver\\n- Confirm object_detector.py handles rgb8 correctly (it does — it converts RGB→BGR for OpenCV)\\n\\n### 3. Resolution and FOV changes\\n- R200 RGB: 1920×1080, 70° HFOV\\n- D435i color: configurable; default 1280×720 or 1920×1080, 69° HFOV\\n- YOLOv8 auto-resizes input — no change needed in model\\n- Verify detection confidence and class accuracy are maintained\\n\\n### 4. End-to-end test\\n- Launch: zenoh-router, demo-world-enhanced (or real robot), zenoh-bridge, detector, detection-logger\\n- Run query_detections.py --format summary and verify detections appear\\n- Run docker compose run --rm test-detection-logging (26 tests must pass)\\n\\n### 5. Update docker-compose.yaml and README\\n- Update detector service command with D435i image key\\n- Update README Data Recording section with correct D435i topic names\\n\\n## Acceptance Criteria\\n- YOLOv8 detector receives frames from D435i (real or simulated)\\n- Detections logged to data/detections/detections.jsonl with correct schema\\n- All 26 tests in test_detection_logging.py pass\\n- README reflects D435i topic names\\n\\n## Blocked By\\n- turtlebot-maze-2mc (SLAM swap must confirm D435i Zenoh key naming)\\n- turtlebot-maze-3ml (Nav2 audit must confirm no topic conflicts)\",\"status\":\"open\",\"priority\":2,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-03-01T13:51:23Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-03-01T13:51:23Z\"}"}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T09:34:20Z","event_type":"closed","id":69,"issue_id":"turtlebot-maze-dgm","new_value":"Closed","old_value":""}
{"actor":"Pantelis Monogioudis","comment":null,"created_at":"2026-03-01T09:39:17Z","event_type":"updated","id":70,"issue_id":"turtlebot-maze-dgm","new_value":"{\"notes\":\"COMPLETED. Work done: updated object_detector.py default --image-key from camera/image_raw (legacy alias) to camera/color/image_raw (canonical D435i/ROS 2 topic). Updated docker-compose.yaml detector command with explicit key. Updated README throughout (architecture diagram, Zenoh key table, CLI docs, Foxglove panel table).\\n\\nRoot cause of no frames during investigation: after multiple docker compose up restarts, zenoh-bridge enters a stale DDS discovery state and stops seeing ROS publishers. Fix: restart zenoh-bridge alone (not the simulation) — it re-discovers all current publishers within seconds.\\n\\nLessons learned:\\n1. zenoh-bridge-ros2dds strips the DDS rt/ prefix: ROS /camera/color/image_raw -\\u003e Zenoh key camera/color/image_raw (no rt/).\\n2. zenoh-bridge stale discovery: if Route Publisher routes disappear after container restarts, restart zenoh-bridge to force re-discovery.\\n3. Lazy ros_gz_bridge: (Lazy 0) means NOT lazy — bridge publishes unconditionally. (Lazy 1) would only publish when a subscriber exists.\\n4. Detection pipeline in empty sim scene returns [] detections — this is correct; YOLO finds no objects in the house geometry.\\n\\nVerification: 23 detection msgs/s on tb/detections, 26/26 tests pass.\"}","old_value":"{\"id\":\"turtlebot-maze-dgm\",\"title\":\"Validate D435i for YOLOv8 object detection\",\"description\":\"## Context\\n\\nOnce the R200→D435i camera swap is complete for SLAM (turtlebot-maze-2mc) and the Nav2 impact is resolved (turtlebot-maze-3ml), the YOLOv8 object detection pipeline also needs to be validated with the D435i.\\n\\nThe current object_detector.py subscribes to the camera image Zenoh key. The D435i publishes on different topic names than the R200, which propagate through zenoh-bridge-ros2dds to different Zenoh keys.\\n\\n## Tasks\\n\\n### 1. Update Zenoh image key\\n- Current default in object_detector.py: --image-key camera/image_raw (or rt/intel_realsense_r200_depth/image_raw)\\n- D435i via realsense2_camera + zenoh-bridge: rt/camera/color/image_raw\\n- Update object_detector.py --image-key default and docker-compose.yaml detector command\\n\\n### 2. Verify image encoding compatibility\\n- R200 RGB: publishes rgb8 or bgr8\\n- D435i color: publishes rgb8 by default from realsense2_camera driver\\n- Confirm object_detector.py handles rgb8 correctly (it does — it converts RGB→BGR for OpenCV)\\n\\n### 3. Resolution and FOV changes\\n- R200 RGB: 1920×1080, 70° HFOV\\n- D435i color: configurable; default 1280×720 or 1920×1080, 69° HFOV\\n- YOLOv8 auto-resizes input — no change needed in model\\n- Verify detection confidence and class accuracy are maintained\\n\\n### 4. End-to-end test\\n- Launch: zenoh-router, demo-world-enhanced (or real robot), zenoh-bridge, detector, detection-logger\\n- Run query_detections.py --format summary and verify detections appear\\n- Run docker compose run --rm test-detection-logging (26 tests must pass)\\n\\n### 5. Update docker-compose.yaml and README\\n- Update detector service command with D435i image key\\n- Update README Data Recording section with correct D435i topic names\\n\\n## Acceptance Criteria\\n- YOLOv8 detector receives frames from D435i (real or simulated)\\n- Detections logged to data/detections/detections.jsonl with correct schema\\n- All 26 tests in test_detection_logging.py pass\\n- README reflects D435i topic names\\n\\n## Blocked By\\n- turtlebot-maze-2mc (SLAM swap must confirm D435i Zenoh key naming)\\n- turtlebot-maze-3ml (Nav2 audit must confirm no topic conflicts)\",\"status\":\"closed\",\"priority\":2,\"issue_type\":\"task\",\"owner\":\"pantelis.monogioudis@aegean.ai\",\"created_at\":\"2026-03-01T13:51:23Z\",\"created_by\":\"Pantelis Monogioudis\",\"updated_at\":\"2026-03-01T14:34:21Z\",\"closed_at\":\"2026-03-01T14:34:21Z\",\"close_reason\":\"Closed\"}"}
