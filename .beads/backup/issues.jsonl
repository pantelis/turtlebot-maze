{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"63d61b64a78fb22a394133e716bbd08fe015ec380e40648a48cecb582a16ad32","created_at":"2026-02-22T03:18:26Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Create a new ROS 2 node (or package) for the environment agent that:\n- Subscribes to all overhead camera feeds\n- Runs perception pipeline on camera images (object detection, robot tracking, ArUco marker detection from overhead view)\n- Maintains internal state of the maze (robot position, obstacle positions, marker locations)\n- Publishes environment observations to the robot agent via the defined communication interface\n- Provides services for on-demand queries from the robot\n- Runs independently of the robot agent process\nThis node is the bridge between the camera network and the physical AI model.","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-0tj","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Build environment agent node","updated_at":"2026-02-22T03:18:26Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"2763715ba038e48b4a7d3e3f9599c4a90075497b16440a1d86c747d8f05870f1","created_at":"2026-02-27T12:36:40Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"stella_vslam ships with a Pangolin-based 3D viewer (socket_publisher + PangolinViewer) that visualizes the SLAM map, keyframes, and camera trajectory in real time. Currently the demo-slam container runs run_slam without the --viewer flag, so there is no visual feedback on SLAM quality.\n\n## What Pangolin Viewer Shows\n\n- 3D point cloud map built by stella_vslam\n- Camera trajectory (green path)\n- Keyframe poses (blue frustums)\n- Current camera pose (red frustum)\n- Tracking state (tracking / lost / initializing)\n\n## Implementation\n\n1. Add the stella_vslam socket_publisher plugin to the Dockerfile.slam build\n2. Pass --viewer socket_publisher to run_slam in the docker-compose.yaml demo-slam command\n3. Run the PangolinViewer client (socket_viewer) — either:\n   a. In the same container alongside run_slam (requires DISPLAY forwarding), OR\n   b. In a separate docker-compose service that connects to the socket_publisher\n4. Configure DISPLAY and /tmp/.X11-unix volume mount in the demo-slam service (same pattern as demo-world-enhanced)\n\n## Reference\n\nstella_vslam viewer docs: https://stella-cv.github.io/docs/en/simple_tutorial.html\nsocket_publisher is the recommended viewer transport when running headless or in containers.\n\n## Notes\n\n- Pangolin viewer is X11-based; needs DISPLAY env var and X11 socket mount\n- The viewer connects to run_slam via a socket (default port 3001)\n- This is independent of the Zenoh transport — viewer data goes directly socket → Pangolin, not through Zenoh\n- Can be combined with turtlebot-maze-es3 (Zenoh storage) for full observability: live Pangolin view + recorded Zenoh data","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-2b7","is_template":0,"issue_type":"feature","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Integrate Pangolin viewer from stella_vslam to visualize Visual SLAM map and trajectory","updated_at":"2026-02-27T12:36:40Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Closed","closed_at":"2026-03-01T14:21:17Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"5d84865112ddea3bfb96c535952e953a060d011d3f06a40bef93341c2ffc9175","created_at":"2026-03-01T13:50:48Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"## Context\n\nThe Intel RealSense R200's RGB camera uses a rolling shutter, which causes geometric distortion during motion and is the root cause of rapid tracking loss in stella_vslam (observed: tracking lost within 5 frames of initialization). The D435i has a global shutter RGB camera and a built-in IMU, making it the appropriate sensor for visual SLAM.\n\nDocumented in: docs/plans/2026-02-21-stella-vslam-plan.md (Camera Model Assessment section)\n\n## Scope\n\n### 1. Zenoh key updates\n- Simulation: image key is rt/intel_realsense_r200_depth/image_raw (from zenoh-bridge-ros2dds)\n- Real robot (D435i): ROS 2 driver publishes camera/color/image_raw and camera/depth/image_rect_raw → bridged as rt/camera/color/image_raw and rt/camera/depth/image_rect_raw\n- Update slam_bridge.py --image-key and --depth-key defaults\n- Update docker-compose.yaml demo-slam command arguments\n\n### 2. Camera config\n- Create slam/config/d435i_rgbd.yaml with real calibrated intrinsics\n- Switch from monocular (feed_monocular_frame) to RGBD mode (feed_RGBD_frame) in run_slam.cc\n- D435i color: fx≈615, fy≈615, cx≈320, cy≈240 at 640×480 (calibrate to confirm)\n- Add depth_factor: 0.001 (D435i depth is in mm, stella_vslam expects metres)\n- Set color_order: RGB (D435i publishes RGB, not BGR)\n\n### 3. run_slam.cc rewrite for RGBD\n- Add depth image stdin channel (second stream after colour frame)\n- Call slam-\u003efeed_RGBD_frame(color, depth, timestamp) instead of feed_monocular_frame\n- Wire format extension: after colour header+pixels, send depth header+pixels (uint16, millimetres)\n\n### 4. slam_bridge.py updates\n- Subscribe to both image and depth Zenoh keys\n- Deserialize depth as 16-bit single-channel image\n- Send paired (colour, depth) frames via stdin pipe\n- Consider IMU subscription for future VINS upgrade\n\n### 5. Simulation compatibility\n- Gazebo SDF model in tb_worlds should publish a simulated depth stream on the D435i topic name\n- Or: add a --simulate flag that uses the existing 320×240 colour-only stream with monocular mode\n\n## Acceptance Criteria\n- stella_vslam initializes and holds tracking for \u003e60 seconds in demo-world-enhanced\n- Pose records appear in tb/slam/pose Zenoh topic (verified via slam-logger JSONL)\n- slam/config/d435i_rgbd.yaml committed with documented calibration source\n- run_slam.cc compiles cleanly and processes RGBD frames from D435i","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-2mc","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"COMPLETED. Root causes found and fixed:\n1. Zenoh key prefix: zenoh-bridge-ros2dds strips the DDS rt/ prefix when publishing to Zenoh. slam_bridge was subscribing to rt/camera/* but bridge publishes to camera/*. Fixed in slam_bridge.py defaults and docker-compose.yaml demo-slam command.\n2. Volume mounts: turtlebot3_bridge.yaml and gz_waffle.sdf.xacro volume-mounted to demo-world-enhanced; ros_gz_bridge now bridges /camera/realsense_d435i/* -\u003e /camera/color/image_raw and /camera/depth/image_rect_raw correctly.\nVerification: demo-slam shows Mode=rgbd, Image key=camera/color/image_raw, Processed 50 frames.","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":1,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Swap R200 with D435i in SLAM pipeline","updated_at":"2026-03-01T14:21:17Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"37901140e6aaf77a6452719eea7372aedbee9155fe7dd2db8ad632c2af9e5e5f","created_at":"2026-02-22T03:18:17Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Define the ROS 2 topic/service API between the robot agent and the environment agent. This includes:\n- Topic schema for environment observations sent to the robot (e.g., global pose corrections, obstacle map updates, suggested waypoints)\n- Topic schema for robot state published to the environment (e.g., intended goals, local sensor data, action status)\n- Service interfaces for on-demand queries (e.g., 'where am I?', 'is path clear to X?')\n- Message type definitions (custom ROS 2 msgs/srvs)\n- Latency and QoS requirements for real-time guidance\nThis is the foundation that all other sub-issues depend on.","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-3a5","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Design agent-environment communication interface","updated_at":"2026-02-22T03:18:17Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"9f0e72b03bbf9dd9f2a3c913fed1188efceceb5d04b71b18ec96550b0b097b4d","created_at":"2026-03-01T13:51:06Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"## Context\n\nBefore swapping the physical camera from R200 to D435i, we need to understand all the places the R200 is referenced in the navigation stack and assess what changes are needed to keep Nav2 functional.\n\n---\n\n## FINDINGS (completed 2026-03-01)\n\n### TL;DR: Nav2 does NOT use the camera. The D435i swap is safe for navigation.\n\n---\n\n### 1. Nav2 sensor sources — camera-free\n\n`tb_worlds/configs/nav2_params.yaml` uses only LiDAR for all costmap layers:\n\n```yaml\nlocal_costmap:\n  voxel_layer:\n    observation_sources: scan   # LaserScan only — no camera\n    scan:\n      topic: /scan\n      data_type: \"LaserScan\"\n\nglobal_costmap:\n  obstacle_layer:\n    observation_sources: scan   # LaserScan only — no camera\n\ncollision_monitor:\n  observation_sources: [\"scan\"] # LaserScan only\n```\n\nAMCL uses scan (LiDAR) for localisation. No voxel layer, no depth camera, no pointcloud source anywhere in nav2_params.yaml. **Zero Nav2 parameter changes are needed for the D435i swap.**\n\n---\n\n### 2. R200 references — complete list\n\n| File | Line(s) | Type | Impact on D435i swap |\n|------|---------|------|----------------------|\n| `tb_worlds/urdf/gz_waffle.sdf.xacro` | 392, 415 | SDF sensor name + topic | Must rename |\n| `tb_worlds/configs/turtlebot3_bridge.yaml` | 53–66 | Gz→ROS bridge topic names | Must rename |\n| `tb_worlds/configs/turtlebot3_bridge.yaml` | 72–76 | Legacy `/camera/image_raw` alias | Keep — vision behaviors depend on it |\n| `tb_autonomy/src/vision_behaviors.cpp` | 27 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\n| `tb_autonomy/python/tb_behaviors/vision.py` | 88 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\n| `tb_autonomy/scripts/test_vision.py` | 31 | Subscribes `/camera/image_raw` | No change needed (uses alias) |\n| `slam/slam_bridge.py` | 262, 268 | `--image-key` / `--depth-key` defaults | Handled by turtlebot-maze-2mc |\n| `slam/test_aruco_detection.py` | 53 | `--image-key` default | Update alongside 2mc |\n| `docker-compose.yaml` | 239–240 | demo-slam command args | Handled by turtlebot-maze-2mc |\n| `docs/plans/` | various | Plan text / code examples | Docs-only, no runtime impact |\n\n---\n\n### 3. TF frames\n\nThe SDF model defines (gz_waffle.sdf.xacro):\n- `camera_link` (line 368) — parent link for the camera assembly\n- `camera_depth_frame` (line 396, via `\u003cgz_frame_id\u003e`) — sensor frame\n\nThe real D435i via `realsense2_camera` driver publishes:\n`camera_link`, `camera_depth_frame`, `camera_depth_optical_frame`,\n`camera_color_frame`, `camera_color_optical_frame`, `camera_imu_frame`\n\n**Frame names match** — `camera_link` and `camera_depth_frame` are identical between the simulated R200 and the real D435i driver. No TF remapping is needed. The additional frames the D435i publishes (optical frames, IMU) are additive and do not break anything.\n\n---\n\n### 4. ROS topic changes needed\n\n| Current (R200 / simulation) | D435i (realsense2_camera) | Users |\n|----------------------------|--------------------------|-------|\n| `/intel_realsense_r200_depth/image_raw` | `/camera/color/image_raw` | SLAM (2mc), detector (dgm) |\n| `/intel_realsense_r200_depth/depth/image_raw` | `/camera/depth/image_rect_raw` | SLAM (2mc) |\n| `/intel_realsense_r200_depth/points` | `/camera/depth/color/points` | Nothing currently — unused |\n| `/camera/image_raw` (alias) | keep as remap or use `/camera/color/image_raw` | vision_behaviors.cpp, vision.py |\n\nThe vision behaviors subscribe to `/camera/image_raw`. With the real D435i the simplest fix is to add a ROS 2 topic remap in the launch file (`/camera/color/image_raw -\u003e /camera/image_raw`) rather than changing all subscriber code.\n\n---\n\n### 5. Simulation model changes required\n\nTo simulate a D435i in Gazebo (cosmetic but good hygiene):\n\n**gz_waffle.sdf.xacro** — rename sensor:\n```xml\n\u003c!-- Before --\u003e\n\u003csensor name=\"intel_realsense_r200_depth\" type=\"rgbd_camera\"\u003e\n  \u003ctopic\u003eintel_realsense_r200_depth\u003c/topic\u003e\n\n\u003c!-- After --\u003e\n\u003csensor name=\"camera_realsense_d435i\" type=\"rgbd_camera\"\u003e\n  \u003ctopic\u003ecamera/realsense_d435i\u003c/topic\u003e\n```\n\n**turtlebot3_bridge.yaml** — update Gz topic names to match:\n```yaml\n- ros_topic_name: \"/camera/color/image_raw\"\n  gz_topic_name: \"/camera/realsense_d435i/image\"\n- ros_topic_name: \"/camera/depth/image_rect_raw\"\n  gz_topic_name: \"/camera/realsense_d435i/depth_image\"\n- ros_topic_name: \"/camera/depth/color/points\"\n  gz_topic_name: \"/camera/realsense_d435i/points\"\n# Keep legacy alias for vision behaviors:\n- ros_topic_name: \"/camera/image_raw\"\n  gz_topic_name: \"/camera/realsense_d435i/image\"\n```\n\nResolution can stay at 320×240 for simulation (fast, already tuned).\n\n---\n\n### 6. Impact assessment summary\n\n| Layer | Impact | Changes needed |\n|-------|--------|----------------|\n| Nav2 (AMCL, costmaps, planners) | **None** | None — LiDAR only |\n| TF frames | **None** | frame names identical |\n| Vision behaviors (HSV/YOLO color detection) | **Minimal** | Add launch file remap `/camera/color/image_raw → /camera/image_raw` |\n| SLAM pipeline | Medium | Handled by turtlebot-maze-2mc |\n| Object detector (Zenoh) | Minimal | Handled by turtlebot-maze-dgm |\n| Simulation model | Cosmetic | Rename SDF sensor + bridge topics |\n\n---\n\n## Required Changes (scoped to this issue)\n\n1. **`tb_worlds/urdf/gz_waffle.sdf.xacro`** — rename sensor from `intel_realsense_r200_depth` to `camera_realsense_d435i`, update `\u003ctopic\u003e`\n2. **`tb_worlds/configs/turtlebot3_bridge.yaml`** — update Gz→ROS topic bridges to D435i names; keep `/camera/image_raw` legacy alias\n3. **Launch file remap** — add `/camera/color/image_raw → /camera/image_raw` remap for real-robot deployment so vision behaviors need no code change\n\n## Investigation Tasks\n\n### 1. Find all R200 references in the repo ✅\n### 2. Nav2 sensor sources ✅\n### 3. TF frame changes ✅\n### 4. ROS topic name changes ✅\n### 5. Simulation model ✅\n\n## Deliverables\n- Written summary of all impacted files with specific line numbers ✅\n- List of Nav2 param changes needed ✅ (none)\n- List of TF frame / topic remapping changes needed ✅\n- Assessment: is the swap a simple topic rename or does it require URDF/SDF model changes? ✅ Primarily topic rename + SDF sensor rename; TF unchanged; Nav2 unaffected","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-3ml","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":1,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"completed","target":"","timeout_ns":0,"title":"Audit R200 usage in Nav2 and assess D435i migration impact","updated_at":"2026-03-01T14:00:07Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Tutorial rewritten: starts from original featureless maze, walks through generic enhancement steps, no PR references. Pushed to eaia PR #46.","closed_at":"2026-02-20T20:05:38Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"a1049206046ad1728c31115442141e6dd003054ecaf1de024ce1d9b0f37966d5","created_at":"2026-02-20T16:47:38Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Write a Mintlify blog post tutorial covering all steps to enhance the maze world: cherry-picking assets from PR #3, creating sim_house_enhanced.sdf.xacro with taller walls and PBR textures, adding ArUco marker spawner, parameterizing launch files with world_name/use_aruco, and adding demo-world-enhanced Docker Compose service. Target location: /home/pantelis.monogioudis/local/web/sites/courses/eaia/src/blog/tutorials/","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-4y3","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Author tutorial blog post: Enhancing the TurtleBot Maze World","updated_at":"2026-02-20T20:05:38Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"f30247d69ffad1ab1ffa9c86667e50f94fef140913d87a7908f5b13e15523fbd","created_at":"2026-02-21T11:54:08Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Roboticists using Claude Code (or any MCP-compatible AI assistant) lack a documentation MCP server for ROS 2, Gazebo Sim, and Nav2. When verifying API behavior (e.g., how ExecuteProcess handles nested lists, what launch arguments gz_spawn_model accepts), the only options are web search or reading installed source — neither is reliable or fast.\n\n## Goal\nBuild an MCP server that indexes and serves ROS 2 ecosystem documentation, enabling AI assistants to look up API references, launch file parameters, message types, and library behavior on demand.\n\n## Scope\n- Index docs.ros.org (launch API, rclpy, rclcpp, message/service/action types)\n- Index Gazebo Sim API docs (gz-sim, SDFormat, gz-transport)\n- Index Nav2 documentation (configuration, plugins, behavior trees)\n- Expose tools: search-docs, get-api-reference, get-message-type, get-launch-args\n- Package as installable MCP server for Claude Code\n\n## Existing Options to Evaluate\n1. **Context7 MCP** (github.com/upstash/context7) — general-purpose library doc server, may already cover some ROS 2 libs\n2. **Library Docs MCP Server** (vikramdse/docs-mcp-server) — generic doc search via Serper API\n3. **Google Developer Knowledge MCP** — pattern reference for doc-serving MCP\n4. **Custom build** — scrape/index ROS 2 doc sites, serve via MCP tools\n\n## Deliverables\n1. Working ROS 2 docs MCP server (MVP: launch API + Nav2 config docs)\n2. Integration test with Claude Code\n3. Blog tutorial in eaia repo (src/blog/tutorials/) documenting the problem, surveying existing options, and showing the solution","design":"# ROS 2 Documentation MCP Server — Design Outline\n\n## Problem Statement\nAI coding assistants (Claude Code, Cursor, etc.) connected to ROS 2 systems via ros-mcp-server can introspect running robots but cannot consult ROS 2 documentation. When a roboticist needs to verify API behavior — e.g., how ExecuteProcess.cmd handles LaunchConfiguration substitutions, or what arguments gz_spawn_model.launch.py accepts — the assistant must fall back to web search or reading raw source code. This is slow, unreliable, and breaks the workflow.\n\n## Architecture\n- Python MCP server using FastMCP or the official MCP Python SDK\n- Documentation ingested from public sources, chunked and indexed\n- Vector or keyword search over indexed docs\n- Tools exposed: search_ros2_docs, get_api_reference, get_message_definition, get_launch_file_args\n\n## Documentation Sources (Priority Order)\n1. **ROS 2 Launch API** — docs.ros.org/en/jazzy/p/launch/\n2. **rclpy API** — docs.ros.org/en/jazzy/p/rclpy/\n3. **Nav2 docs** — docs.nav2.org\n4. **SDFormat spec** — sdformat.org\n5. **Gazebo Sim API** — gazebosim.org/api\n6. **ROS 2 tutorials** — docs.ros.org/en/jazzy/Tutorials/\n\n## Existing Options Survey\n| Option | Pros | Cons |\n|--------|------|------|\n| Context7 MCP | Ready-made, curated DB, Claude plugin | May not cover ROS 2; proprietary index |\n| Library Docs MCP (vikramdse) | Generic, uses Serper search | External API dependency; not ROS-specific |\n| Google Dev Knowledge MCP | Good pattern reference | Google docs only |\n| Custom build | Full control, offline-capable, ROS 2-specific | Build effort |\n\n## Implementation Plan\n### Phase 1: Evaluate Context7 coverage\n- Install Context7 MCP server\n- Test if ROS 2 launch, rclpy, Nav2 docs are indexed\n- If coverage is sufficient, document setup and skip custom build\n\n### Phase 2: Custom MCP server (if needed)\n- Scrape/download ROS 2 doc pages (HTML → markdown)\n- Chunk and index with lightweight search (e.g., tantivy, sqlite-fts5, or embedded vectors)\n- Implement MCP tools: search_ros2_docs, get_api_reference\n- Package as pip-installable, add to Claude Code via claude mcp add\n\n### Phase 3: Blog tutorial\n- Write tutorial in eaia repo: src/blog/tutorials/ros2-docs-mcp/\n- Document the problem (roboticists + AI assistants + missing docs)\n- Survey all options (Context7, Library Docs MCP, Google Dev Knowledge, custom)\n- Show working setup with Claude Code\n- Include practical examples (verifying launch API behavior, looking up message types)","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-5k3","is_template":0,"issue_type":"feature","last_activity":null,"metadata":"{}","mol_type":"","notes":"# Landscape: Documentation MCP Servers for Roboticists\n\n## The Gap\n\nRoboticists using Claude Code with ros-mcp-server can introspect a running ROS 2 system (topics, services, actions, parameters) but have no way to consult API documentation. When verifying API behavior — e.g., how `ExecuteProcess.cmd` handles nested lists with `LaunchConfiguration` substitutions, or what arguments `gz_spawn_model.launch.py` accepts — the only options are:\n\n1. **Web search** — unreliable, slow, may return outdated results\n2. **Reading installed source code** — accurate but requires knowing where to look\n3. **Claude's training data** — covers ROS 2 through Jazzy but can be stale or wrong on edge cases\n\nNone of these are fast or integrated into the AI assistant workflow.\n\n## Existing Options\n\n### 1. Context7 MCP Server (Most Promising)\n- **Repo**: https://github.com/upstash/context7\n- **What it does**: Serves up-to-date, version-specific library documentation directly into the LLM context window. Pulls docs and code examples from a curated database.\n- **Install for Claude Code**: `claude mcp add --scope user context7 -- npx -y @upstash/context7-mcp`\n- **Tools**: `resolve-library-id` (find library), `query-docs` (fetch docs for a library)\n- **ROS 2 coverage**: Unknown — needs evaluation. Libraries can potentially be added to their index.\n- **Pros**: Ready-made, curated, version-aware, Claude plugin available\n- **Cons**: Proprietary index; may not cover ROS 2 ecosystem; requires npm/npx\n\n### 2. Library Docs MCP Server\n- **Registry**: https://lobehub.com/mcp/vikramdse-docs-mcp-server\n- **What it does**: Generic MCP server for searching and fetching documentation for popular libraries using the Serper API.\n- **Pros**: Generic approach works for any library with online docs\n- **Cons**: External API dependency (Serper); not ROS-specific; search quality depends on web indexing\n\n### 3. Google Developer Knowledge MCP\n- **Announcement**: https://developers.googleblog.com/introducing-the-developer-knowledge-api-and-mcp-server/\n- **What it does**: Serves Google's official developer documentation via MCP. Provides a canonical, machine-readable gateway.\n- **Pros**: Shows the gold-standard pattern for doc-serving MCP; structured API\n- **Cons**: Google docs only — not applicable to ROS 2 directly; useful as architecture reference\n\n### 4. Custom ROS 2 Docs MCP Server (Build It)\n- **What it would do**: Scrape/index docs.ros.org, Nav2, Gazebo Sim, SDFormat docs. Expose search and lookup tools via MCP.\n- **Target doc sources**: launch API, rclpy, rclcpp, message types, Nav2 config, SDFormat spec, Gazebo Sim API\n- **Tools**: `search_ros2_docs`, `get_api_reference`, `get_message_definition`, `get_launch_file_args`\n- **Pros**: Full control, offline-capable, ROS 2-specific, covers the full ecosystem\n- **Cons**: Build and maintenance effort; need to handle doc updates across ROS 2 releases\n\n## Recommendation\n\nStart with Phase 1: evaluate Context7 coverage of ROS 2. If it covers rclpy/launch/Nav2, document setup and skip custom build. If not, proceed to Phase 2 (custom build) using Google Dev Knowledge MCP as architecture reference.","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Build ROS 2 Documentation MCP Server","updated_at":"2026-02-21T11:55:58Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"All three issues fixed and verified: (1) devcontainer service pointed to dev, (2) zenoh-bridge updated to v1.7.2 from new repo, (3) only install bridge deb to avoid zenohd dependency. Docker build succeeds and container starts with correct workspace mount.","closed_at":"2026-02-19T13:58:57Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"07e63a76407a7508cf260d24dc0d0fd2a103760a47973eebd1776070cdbc189b","created_at":"2026-02-19T13:28:10Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"When launching the Docker dev container via VS Code (Dev Container: Existing Docker Compose), two errors occur: (1) VS Code dialog: Workspace does not exist - Please select another workspace to open. (2) Terminal error: The terminal process failed to launch: Starting directory (cwd) /workspaces/turtlebot-maze does not exist. The devcontainer.json or docker-compose config references /workspaces/turtlebot-maze as the workspace folder, but that path is not created or mounted inside the container. Fix: Ensure the volume mount in docker-compose.yml maps the project source to /workspaces/turtlebot-maze, or update devcontainer.json workspaceFolder to match the actual mount point.","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-8eg","is_template":0,"issue_type":"bug","last_activity":null,"metadata":"{}","mol_type":"","notes":"Root cause: Two issues - (1) devcontainer.json pointed to wrong service (demo-world → dev) and volume mount path was incorrect, (2) zenoh-bridge-ros2dds v1.2.1 .deb no longer exists on GitHub (repo renamed to zenoh-plugin-ros2dds, releases now ship as zip archives). Fix: Updated .devcontainer files to use dev service with correct mount, and updated Dockerfile.gpu to use zenoh-plugin-ros2dds v1.7.2 with new zip-based download format.","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":1,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Dev container workspace path /workspaces/turtlebot-maze does not exist","updated_at":"2026-02-19T13:58:57Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"ArUco marker detection verified working. DICT_4X4_100 detects marker ID 80 at 98% rate (53/54 frames). Fixed model SDFs to use flat plane PBR textures, corrected marker positions inside house bounds, updated test script.","closed_at":"2026-02-22T01:54:50Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"749a57cf25f2dc8296b472d59cd26121aad2823c7215a4b6478e39cbde56693d","created_at":"2026-02-22T00:25:26Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Verify that the ArUco markers (IDs 60 and 80) spawned in demo-world-enhanced are visible and detectable by the TurtleBot's camera. Drive the robot to marker locations and confirm detection using OpenCV's ArUco detector or a simple subscriber. This validates the markers placed by aruco_marker_spawner.launch.py are correctly positioned and oriented for camera-based pose estimation.","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-b2j","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Test ArUco marker detection with camera","updated_at":"2026-02-22T01:54:50Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Closed","closed_at":"2026-03-01T14:34:21Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"db264638e84788ba82dd18d20c669c9dbc3dbf904f7a9469c4e5e0f514092ecf","created_at":"2026-03-01T13:51:23Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"## Context\n\nOnce the R200→D435i camera swap is complete for SLAM (turtlebot-maze-2mc) and the Nav2 impact is resolved (turtlebot-maze-3ml), the YOLOv8 object detection pipeline also needs to be validated with the D435i.\n\nThe current object_detector.py subscribes to the camera image Zenoh key. The D435i publishes on different topic names than the R200, which propagate through zenoh-bridge-ros2dds to different Zenoh keys.\n\n## Tasks\n\n### 1. Update Zenoh image key\n- Current default in object_detector.py: --image-key camera/image_raw (or rt/intel_realsense_r200_depth/image_raw)\n- D435i via realsense2_camera + zenoh-bridge: rt/camera/color/image_raw\n- Update object_detector.py --image-key default and docker-compose.yaml detector command\n\n### 2. Verify image encoding compatibility\n- R200 RGB: publishes rgb8 or bgr8\n- D435i color: publishes rgb8 by default from realsense2_camera driver\n- Confirm object_detector.py handles rgb8 correctly (it does — it converts RGB→BGR for OpenCV)\n\n### 3. Resolution and FOV changes\n- R200 RGB: 1920×1080, 70° HFOV\n- D435i color: configurable; default 1280×720 or 1920×1080, 69° HFOV\n- YOLOv8 auto-resizes input — no change needed in model\n- Verify detection confidence and class accuracy are maintained\n\n### 4. End-to-end test\n- Launch: zenoh-router, demo-world-enhanced (or real robot), zenoh-bridge, detector, detection-logger\n- Run query_detections.py --format summary and verify detections appear\n- Run docker compose run --rm test-detection-logging (26 tests must pass)\n\n### 5. Update docker-compose.yaml and README\n- Update detector service command with D435i image key\n- Update README Data Recording section with correct D435i topic names\n\n## Acceptance Criteria\n- YOLOv8 detector receives frames from D435i (real or simulated)\n- Detections logged to data/detections/detections.jsonl with correct schema\n- All 26 tests in test_detection_logging.py pass\n- README reflects D435i topic names\n\n## Blocked By\n- turtlebot-maze-2mc (SLAM swap must confirm D435i Zenoh key naming)\n- turtlebot-maze-3ml (Nav2 audit must confirm no topic conflicts)","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-dgm","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"COMPLETED. Work done: updated object_detector.py default --image-key from camera/image_raw (legacy alias) to camera/color/image_raw (canonical D435i/ROS 2 topic). Updated docker-compose.yaml detector command with explicit key. Updated README throughout (architecture diagram, Zenoh key table, CLI docs, Foxglove panel table).\n\nRoot cause of no frames during investigation: after multiple docker compose up restarts, zenoh-bridge enters a stale DDS discovery state and stops seeing ROS publishers. Fix: restart zenoh-bridge alone (not the simulation) — it re-discovers all current publishers within seconds.\n\nLessons learned:\n1. zenoh-bridge-ros2dds strips the DDS rt/ prefix: ROS /camera/color/image_raw -\u003e Zenoh key camera/color/image_raw (no rt/).\n2. zenoh-bridge stale discovery: if Route Publisher routes disappear after container restarts, restart zenoh-bridge to force re-discovery.\n3. Lazy ros_gz_bridge: (Lazy 0) means NOT lazy — bridge publishes unconditionally. (Lazy 1) would only publish when a subscriber exists.\n4. Detection pipeline in empty sim scene returns [] detections — this is correct; YOLO finds no objects in the house geometry.\n\nVerification: 23 detection msgs/s on tb/detections, 26/26 tests pass.","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Validate D435i for YOLOv8 object detection","updated_at":"2026-03-01T14:39:17Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Demo verified end-to-end: /navigate slash command connects to rosbridge, verifies Nav2, sends NavigateToPose goal to location3, robot arrives within 0.1m of target","closed_at":"2026-02-19T14:33:09Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"73747a9f744d332da4c17ce37b4b52584a975fe9c721ba61940ac4dcd8609a4e","created_at":"2026-02-19T14:13:14Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Create a demo showcasing ros-mcp-server's ability to command the TurtleBot to navigate to a user-specified location using Nav2.\n\n## Context\nThe ros-mcp-server is already integrated (.mcp.json, rosbridge service on port 9090). The simulation world (sim_house) has 4 predefined locations in sim_house_locations.yaml:\n- location1: (-1.0, -0.5, -2.25)\n- location2: (0.5, 4.0, 0.785)\n- location3: (4.0, 0.5, 1.571)\n- location4: (2.75, 2.5, -1.571)\nRobot starts at (0.6, 0.6, 0.0).\n\n## Demo Flow\n1. Start the simulation: `docker compose up demo-world`\n2. Start rosbridge: `docker compose up rosbridge`\n3. Use ros-mcp-server tools from Claude Code to:\n   a. `connect_to_robot` (127.0.0.1:9090)\n   b. `get_topics` / `get_actions` to verify Nav2 is running\n   c. `subscribe_once` to /amcl_pose to confirm robot's current position\n   d. `send_action_goal` on /navigate_to_pose (nav2_msgs/action/NavigateToPose) to move the robot to a user-chosen location\n   e. `get_action_status` to monitor navigation progress\n   f. `subscribe_once` to /amcl_pose to confirm arrival\n\n## Permissions\nsend_action_goal is intentionally NOT pre-approved in .claude/settings.local.json — user must grant permission each time (safety gate for robot commands). Consider whether to add it to the allow list for the demo.\n\n## Acceptance Criteria\n- Document the demo steps in a script or README section\n- Verify the full flow works end-to-end in the sim_house world\n- Show the robot navigating from its start pose to at least one of the 4 known locations\n- Capture the ros-mcp-server tool calls and their responses as part of the demo documentation","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-dwg","is_template":0,"issue_type":"feature","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Demo: navigate robot to desired location via ros-mcp-server","updated_at":"2026-02-19T14:33:09Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Part A (foxglove-bridge) already done. Part B: zenoh-storage.json5 (in-memory storage-manager), detection_logger.py (persistent JSONL), query_detections.py (query/export/CSV/summary). SLAM storage split into turtlebot-maze-m3n.","closed_at":"2026-03-01T00:28:10Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"9088c338b4b09277831ba7ab5dcec035453991b09b61352587bb4ea9f4d53a7e","created_at":"2026-02-27T12:35:57Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Add Zenoh-native message recording to the project using zenoh-plugin-storage-manager AND connect the live SLAM data stream to the team Foxglove dashboard at https://app.foxglove.dev/ai-for-robotics/dashboard for real-time visualization of VSLAM pose, trajectory, and features.\n\n## Architecture\n\n```\nROS 2 Nodes (nav2, Gazebo, behavior tree)\n       ↓ DDS\nzenoh-bridge-ros2dds  →  Zenoh key expressions (rt/camera/*, rt/odom, ...)\n       ↓                         ↓\nzenohd + storage-manager    foxglove-bridge service (NEW)\n       ↓                         ↓\nRocksDB backend           Foxglove WebSocket (ws://localhost:8765)\n       ↓                         ↓\nQuery API (replay)        https://app.foxglove.dev/ai-for-robotics/dashboard\n```\n\n## Part A — Foxglove Live Visualization (Priority)\n\n### How to connect SLAM pose/trajectory/features to Foxglove\n\n**Option 1 — foxglove-bridge in ROS 2 container (recommended)**\n\nfoxglove-bridge is a ROS 2 node that auto-subscribes all topics and serves\nthem over WebSocket on port 8765:\n\n```bash\n# In demo-world-enhanced container\nros2 run foxglove_bridge foxglove_bridge\n```\n\nAdd a `foxglove-bridge` service to docker-compose.yaml:\n```yaml\nfoxglove-bridge:\n  extends: overlay\n  ports:\n    - \"8765:8765\"\n  command: ros2 run foxglove_bridge foxglove_bridge\n```\n\nTopics automatically visualizable in Foxglove 3D panel:\n- `/amcl_pose` → PoseWithCovariance (Nav2 localization)\n- `/map` → OccupancyGrid (maze map)\n- `/scan` → LaserScan (LIDAR)\n- `/camera/image_raw` → Image\n- `/tf`, `/tf_static` → coordinate frames\n\nFor tb/slam/pose (Zenoh, not ROS), a bridge service is needed (see Option 2).\n\n**Option 2 — foxglove-sdk publisher in slam_bridge.py (for tb/slam/pose)**\n\nThe SLAM pose (tb/slam/pose JSON) is Zenoh-only — not in ROS 2 DDS.\nA foxglove-sdk server in slam_bridge.py publishes it directly to Foxglove:\n\n```python\n# Install: pip install foxglove-sdk\nimport foxglove\nfrom foxglove.schemas import PoseInFrame, Pose, Vector3, Quaternion, Timestamp\n\nserver = foxglove.start_server(host=\"0.0.0.0\", port=8766)\n\n# In _read_poses(), after parsing JSON pose matrix:\nimport json, math\ndata = json.loads(line)\nmat = data[\"pose\"]  # 12 elements of 4x4 matrix (row-major)\nfoxglove.log(\"/slam/pose\", PoseInFrame(\n    timestamp=Timestamp.now(),\n    frame_id=\"odom\",\n    pose=Pose(\n        position=Vector3(x=mat[3], y=mat[7], z=mat[11]),\n        orientation=rotation_matrix_to_quaternion(mat),\n    )\n))\n```\n\nFor the trajectory (accumulated poses), use SceneUpdate with LinePrimitive,\nor publish each pose as PoseInFrame and use the Foxglove \"Poses\" panel.\n\nConnect Foxglove to ws://localhost:8766 to see SLAM data.\nConnect Foxglove to ws://localhost:8765 to see all ROS 2 topics.\n\n**Connecting to app.foxglove.dev:**\n\nThe team dashboard at https://app.foxglove.dev/ai-for-robotics/dashboard\ncan connect live when the robot host is reachable:\n1. Open the Foxglove app\n2. Click \"Open connection\" → \"Foxglove WebSocket\"\n3. Enter ws://\u003cHOST_IP\u003e:8765 (ROS topics) or ws://\u003cHOST_IP\u003e:8766 (SLAM)\n4. Save the layout to the team dashboard\n\nFor cloud access (when host is behind NAT), use foxglove-relay or\nngrok to tunnel port 8765/8766.\n\n**Panels to configure in the 3D panel:**\n- `/map` → show OccupancyGrid as maze background\n- `/amcl_pose` → robot position (Nav2)\n- `/slam/pose` → SLAM estimated pose (different color)\n- `/scan` → LIDAR point cloud ring\n- `/camera/image_raw` → camera feed inset\n- `/tf` → coordinate frame axes\n\n## Part B — Zenoh Storage / RocksDB Recording\n\n### Implementation Steps\n\n1. Configure zenoh-router container to load storage-manager plugin with RocksDB backend\n2. Define storage key expressions:\n   - `tb/**` — SLAM pose, status, ArUco detections\n   - `intel_realsense_r200_depth/**` — camera and depth images\n   - `rt/odom`, `rt/scan`, `rt/amcl_pose`, `rt/cmd_vel` — navigation telemetry\n3. Mount a host volume into the zenoh-router container for persistent storage\n4. Write a Python replay/export script using session.get() with _time=[start..end] selectors\n5. Export to MCAP for offline Foxglove playback (mcap-python library)\n\n### Query API (replay)\n\n```python\nsession = zenoh.open(conf)\nresults = session.get(\"tb/**?_time=[2026-02-27T10:00:00..2026-02-27T10:10:00]\")\nfor sample in results:\n    print(sample.key_expr, sample.timestamp, len(bytes(sample.payload)), \"bytes\")\n```\n\n### MCAP Export for offline Foxglove\n\n```python\nfrom mcap.writer import Writer\n# Write each stored sample as a channel/message in MCAP\n# Foxglove opens MCAP files natively — drag-and-drop into app.foxglove.dev\n```\n\n## References\n\n- foxglove-bridge: https://github.com/foxglove/ros-foxglove-bridge\n- foxglove-sdk Python: https://docs.foxglove.dev/docs/sdk\n- Foxglove PoseInFrame schema: https://docs.foxglove.dev/docs/visualization/message-schemas/pose-in-frame\n- zenoh-plugin-storage-manager: https://github.com/eclipse-zenoh/zenoh-backend-rocksdb\n- MCAP format: https://mcap.dev","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-es3","is_template":0,"issue_type":"feature","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Implement Zenoh storage for message traceability and dataset generation","updated_at":"2026-03-01T00:28:10Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Fixed in PR #10 — quoted all special characters in Mermaid node labels and subgraph titles. Auto-merge enabled.","closed_at":"2026-02-21T12:26:29Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"7d77308b4519cf8182989e91dcc62ef2c1574b81c2c8878f746d62d0b3c3e8d4","created_at":"2026-02-21T12:24:46Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"The System Overview section in README.md contains a Mermaid diagram that fails to render on GitHub with a lexical error:\n\n```\nLexical error on line 3. Unrecognized text.\n...AM[/camera/image_raw] NAV[Nav2 S\n-----------------------^\n```\n\nGitHub's Mermaid renderer is choking on the diagram syntax — likely special characters in node labels (forward slashes in topic names like `/camera/image_raw`, or spaces/special chars in labels). Need to fix the Mermaid syntax so it renders correctly on GitHub.","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-iyw","is_template":0,"issue_type":"bug","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Fix broken Mermaid diagram in README System Overview","updated_at":"2026-02-21T12:26:29Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"a8aecd66620dbb4d9a4e0638732d951b4722ddc4ea678e7275c798b0bc3c4b32","created_at":"2026-02-22T03:18:30Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Implement the physical AI model that the environment agent maintains:\n- Real-time digital twin of the maze state built from camera observations\n- Robot state estimation (position, velocity, heading) from overhead tracking\n- Dynamic occupancy grid updated from camera feeds (not just static map)\n- Object/obstacle tracking and prediction (if dynamic obstacles are introduced)\n- Evaluate approaches: classical CV + state estimation vs. learned world models (e.g., neural scene representations)\n- The model should provide the environment agent with predictive capability (e.g., 'robot will collide in N seconds at current trajectory')\n- Consider integration with NVIDIA Isaac or similar physical AI frameworks","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-j6n","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Integrate physical AI / world model into environment agent","updated_at":"2026-02-22T03:18:30Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"64df95a1697a9a5020b5064a1feb718c72caa044db75897436a954544c83ef14","created_at":"2026-02-22T02:59:19Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Decouple the robot agent from the environment so the environment becomes 'smart' — assisted by a separate agent that is aware of a physical AI model. The physical AI model is driven by a network of overhead cameras that observe the maze.\n\nKey architectural changes:\n- **Agent-environment decoupling**: The TurtleBot agent operates independently, receiving guidance/observations from the environment agent rather than relying solely on its onboard sensors\n- **Smart environment agent**: A new agent that processes the overhead camera network feeds and maintains a physical AI model (digital twin / world model) of the maze state\n- **Camera network**: Multiple fixed cameras overlooking the maze provide a bird's-eye or multi-angle view, enabling the environment agent to track robot position, obstacles, and maze layout\n- **Physical AI model**: The environment agent uses camera feeds to build/update a real-time physical model of the maze — this could include occupancy, object positions, dynamic obstacles, and robot state estimation\n- **Communication interface**: Define the API/topics between the environment agent and the robot agent (e.g., suggested waypoints, obstacle alerts, global localization corrections)\n\nThis enables scenarios where the robot has limited onboard sensing but benefits from an omniscient environment that can guide it through the maze.","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-jww","is_template":0,"issue_type":"feature","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Decouple agent from environment with camera-network physical AI","updated_at":"2026-02-22T02:59:19Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Fixed: added runServices: [dev] to devcontainer.json. Root cause was VSCode starting all compose services (including both demo-world and demo-world-enhanced) by default, each launching Gazebo and RViz2.","closed_at":"2026-02-27T00:19:35Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"5d64201bb6c63459ef9f2d64d3ecee31c849cee3fb22ff66621c9140cfe14d4b","created_at":"2026-02-27T00:16:59Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"When opening the project in a VSCode Remote Container (Dev Container), two instances of Gazebo and RViz2 launch instead of one.\n\n**Symptoms:**\n- Two Gazebo windows appear simultaneously on session start\n- Two RViz2 windows appear simultaneously on session start\n- Likely causes resource contention and confusing state\n\n**Suspected causes:**\n- Docker compose service(s) and the VSCode devcontainer postStartCommand / postCreateCommand may both be triggering the launch\n- The devcontainer.json may have a launch command that conflicts with an auto-starting docker compose service\n- A ROS 2 launch file may be sourced twice (e.g., via .bashrc + explicit entrypoint)\n- Two devcontainer configurations or compose overrides both starting the simulation\n\n**Investigation steps:**\n1. Check devcontainer.json for postStartCommand or postCreateCommand that launches Gazebo/RViz2\n2. Check docker-compose.yml for services set to restart or autostart that also run the simulation\n3. Check entrypoint scripts for duplicate launch calls\n4. Check if VSCode opens the container AND runs docker compose up simultaneously\n\n**Expected behavior:**\n- Exactly one Gazebo instance and one RViz2 instance launch when the project is opened in VSCode Remote Container","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-kzv","is_template":0,"issue_type":"bug","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":1,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Fix duplicate Gazebo and RViz2 instances when opening project in VSCode remote container","updated_at":"2026-02-27T00:19:35Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"7735ccd1e127921cf3f437f5cce450dc43f7db12325bae32797eb9e0dda5e7ab","created_at":"2026-03-01T00:16:41Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Configure Zenoh storage-manager to record SLAM data streams for dataset generation and offline Foxglove playback.\n\n## Key expressions to store\n- `tb/slam/pose` — SLAM estimated pose (4x4 matrix JSON from slam_bridge.py)\n- `tb/slam/status` — SLAM status messages\n- `intel_realsense_r200_depth/image_raw` — RGB camera frames (optional, large)\n- `intel_realsense_r200_depth/depth/image_raw` — depth frames (optional, large)\n\n## Tasks\n1. Extend the Zenoh storage config (from turtlebot-maze-es3 object detection work) to add SLAM key expressions\n2. Add slam-specific storage volume (separate dir from detections: /data/slam)\n3. Write a Python replay script that queries slam poses over a time range and reconstructs the trajectory\n4. Export to MCAP format for offline Foxglove playback (mcap-python library)\n\n## References\n- slam_bridge.py publishes to `tb/slam/pose` as JSON\n- Zenoh query API: session.get('tb/slam/pose?_time=[start..end]')\n- MCAP: https://mcap.dev\n- Depends on: zenoh-storage service from turtlebot-maze-es3","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-m3n","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Zenoh storage recording for SLAM pose and trajectory","updated_at":"2026-03-01T00:16:41Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"PR #12 implements standalone stella_vslam Visual SLAM demo with Zenoh transport. Docker build + manual testing needed.","closed_at":"2026-02-21T13:11:07Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"fc4edec8650121ff25eda6471f109f0de352d6228458b3e3fd28de590dbb3773","created_at":"2026-02-21T12:36:45Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"PR #3 (oscarpoudel:updated_maze_plus_slam) includes an install script for stella_vslam at `docker/install_stella_vslam.sh`. The claim is that stella_vslam can run Visual SLAM on the TurtleBot in the maze world. We need to evaluate and integrate this.\n\n## Background\n- [stella_vslam](https://github.com/stella-cv/stella_vslam) is a Visual SLAM framework (fork of OpenVSLAM)\n- PR #3 includes a Jazzy-compatible ROS 2 wrapper: `oscarpoudel/stella_vslam_ros2_Jazzy`\n- The install script builds: iridescence (viewer), stella_vslam (core), iridescence_viewer, and the ROS 2 wrapper\n- The enhanced maze world (`demo-world-enhanced`) with textured walls should provide better visual features for VSLAM\n\n## Tasks\n\n### 1. Cherry-pick and evaluate the install script\n```bash\ngit checkout pr3-temp -- docker/install_stella_vslam.sh\n```\n- Review the script for correctness and security (hardcoded paths to `~/lib`, `~/ros2_ws`)\n- Check if `oscarpoudel/stella_vslam_ros2_Jazzy` repo exists and is maintained\n- Check stella_vslam compatibility with ROS 2 Jazzy and gz-sim\n\n### 2. Dockerize the build\n- The install script uses `~/lib` and `~/ros2_ws` — needs adaptation for our Docker multi-stage build\n- Consider a new Dockerfile stage or separate Dockerfile for stella_vslam\n- stella_vslam needs camera images — verify the Gazebo camera topic is compatible\n\n### 3. Test with enhanced maze world\n- Launch `demo-world-enhanced` (textured walls provide visual features for VSLAM)\n- Run stella_vslam with the TurtleBot's camera feed\n- Verify it can build a map and localize\n- Compare SLAM performance: original featureless walls vs enhanced textured walls\n\n### 4. Add Docker Compose service\n- Add `demo-slam` or similar service if stella_vslam works\n- Document launch procedure in README\n\n## References\n- PR #3: https://github.com/pantelis/turtlebot-maze/pull/3\n- stella_vslam: https://github.com/stella-cv/stella_vslam\n- Jazzy wrapper (claimed): https://github.com/oscarpoudel/stella_vslam_ros2_Jazzy\n- Install script in PR #3: `docker/install_stella_vslam.sh`","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-m5e","is_template":0,"issue_type":"feature","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Test stella_vslam Visual SLAM with enhanced maze world","updated_at":"2026-02-21T13:11:07Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"5299ddc2b34d3a2d96ca6df2a5289e3d0b54cc5579c6d47739c55b58d11d12b5","created_at":"2026-02-22T03:18:38Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Create a full integration test that validates the decoupled architecture:\n- Launch enhanced world with overhead cameras + robot\n- Start environment agent with physical AI model\n- Start robot agent configured to accept environment guidance\n- Robot navigates the maze using a combination of onboard sensing and environment guidance\n- Verify: environment agent correctly tracks robot, provides accurate observations\n- Verify: robot agent uses environment data to improve navigation (fewer collisions, faster completion)\n- Compare performance metrics: standalone vs environment-assisted navigation\n- Docker compose service definitions for the full stack","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-peq","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"End-to-end integration test: environment-assisted maze navigation","updated_at":"2026-02-22T03:18:38Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"abffc5c54e0b17e0a21574fb79d1a9099654a76ed72eaf9fa3b55e29dcfaaeed","created_at":"2026-02-22T03:18:23Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Add a network of fixed cameras overlooking the maze in the Gazebo simulation:\n- Determine optimal camera placement for full maze coverage (bird's-eye and/or multi-angle)\n- Add camera sensor models to sim_house_enhanced.sdf.xacro (or a new world variant)\n- Configure camera parameters (resolution, FOV, frame rate) appropriate for tracking\n- Publish camera feeds on ROS 2 topics (one per camera)\n- Verify full maze coverage with no blind spots\n- Consider both single overhead camera and multi-camera configurations\nStart with simulation; physical camera setup is a separate future task.","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-qob","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Set up overhead camera network in Gazebo","updated_at":"2026-02-22T03:18:23Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Closed","closed_at":"2026-02-27T12:46:06Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"6956c80c508c795029f8d16fb1680cde4a5c0ba7089d11249ac31e3e2e13f746","created_at":"2026-02-27T12:34:21Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"slam_bridge.py writes camera frames as PNG files to a temp directory and run_slam polls that directory for new files. run_slam reads files before the write is complete, causing 'libpng error: Read Error' on every frame. As a result feed_monocular_frame() always returns null and tb/slam/pose is never published.\n\n**Root Cause:**\nThe file-based IPC between slam_bridge.py and run_slam has no write-completion guarantee. The PNG write is not atomic and run_slam picks up partial files.\n\n**Fix:**\nEliminate the temp-file transport entirely. run_slam should receive raw image bytes directly — either via:\n- A Zenoh subscriber inside run_slam (subscribe to the same image key slam_bridge subscribes to), OR\n- A shared memory / pipe mechanism that provides write-completion semantics\n\nThe frame directory flag (-d) in run_slam was designed for file-based input, but since we already have Zenoh in the pipeline, run_slam should subscribe to Zenoh directly and process frames in the callback without any intermediate file I/O.\n\n**Impact:** tb/slam/pose is never published, making the entire Visual SLAM pipeline non-functional.\n\n**Affected files:**\n- slam/slam_bridge.py — writes PNG files, passes -d flag to run_slam\n- slam/run_slam.cc — polls frame directory, reads PNG files with cv::imread","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-szq","is_template":0,"issue_type":"bug","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":1,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Fix slam_bridge PNG race condition: pass frames via Zenoh, not temp files","updated_at":"2026-02-27T12:46:06Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"","closed_at":null,"closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"3754782f689c182537946c36e385a14ac98322fe1aff4a063eb606bb694dc565","created_at":"2026-02-22T03:18:34Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Modify the existing TurtleBot autonomy stack to operate in the decoupled architecture:\n- Subscribe to environment agent observations (global pose, waypoint suggestions, obstacle alerts)\n- Integrate environment data into Nav2 planning (e.g., as an external costmap layer or goal source)\n- Support fallback to onboard-only sensing when environment agent is unavailable\n- Behavior tree modifications: add conditions/actions for environment-assisted navigation\n- The robot should still function autonomously but perform better with environment assistance\n- Maintain backward compatibility with current standalone demo","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-t84","is_template":0,"issue_type":"task","last_activity":null,"metadata":"{}","mol_type":"","notes":"","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"open","target":"","timeout_ns":0,"title":"Refactor robot agent to accept environment guidance","updated_at":"2026-02-22T03:18:34Z","waiters":"","wisp_type":"","work_type":""}
{"acceptance_criteria":"","actor":"","agent_state":"","assignee":null,"await_id":"","await_type":"","close_reason":"Fixed: changed sensor type from camera to depth (matching upstream nav2_minimal_tb3_sim), corrected bridge topic mapping to use base Gazebo topic for images. PR #7 verified with data flowing on all expected topics at 5Hz.","closed_at":"2026-02-19T16:52:09Z","closed_by_session":"","compacted_at":null,"compacted_at_commit":null,"compaction_level":0,"content_hash":"4c1f51fef56d71d936649593860d84089748637ff7706357a0696273b594d66e","created_at":"2026-02-19T14:53:59Z","created_by":"Pantelis Monogioudis","crystallizes":0,"defer_until":null,"description":"Cannot see frames from the RealSense camera in RViz2. Need to diagnose why — likely a TF, topic, or driver configuration issue.","design":"","due_at":null,"ephemeral":0,"estimated_minutes":null,"event_kind":"","external_ref":null,"hook_bead":"","id":"turtlebot-maze-uqk","is_template":0,"issue_type":"bug","last_activity":null,"metadata":"{}","mol_type":"","notes":"Fix verified: all 3 ROS topics (/intel_realsense_r200_depth/image_raw, /points, /camera/image_raw) now have data flowing at ~5Hz. Root cause was sensor type=camera (needed type=depth) and bridge subscribed to wrong Gazebo topic names. The depth_image subtopic does not publish with type=depth, but the base topic carries 32FC1 depth data which is sufficient.","original_size":null,"owner":"pantelis.monogioudis@aegean.ai","payload":"","pinned":0,"priority":2,"quality_score":null,"rig":"","role_bead":"","role_type":"","sender":"","source_repo":"","source_system":"","spec_id":"","status":"closed","target":"","timeout_ns":0,"title":"Investigate missing RealSense camera frames in RViz2","updated_at":"2026-02-19T16:52:09Z","waiters":"","wisp_type":"","work_type":""}
