{"id":"turtlebot-maze-0tj","title":"Build environment agent node","description":"Create a new ROS 2 node (or package) for the environment agent that:\n- Subscribes to all overhead camera feeds\n- Runs perception pipeline on camera images (object detection, robot tracking, ArUco marker detection from overhead view)\n- Maintains internal state of the maze (robot position, obstacle positions, marker locations)\n- Publishes environment observations to the robot agent via the defined communication interface\n- Provides services for on-demand queries from the robot\n- Runs independently of the robot agent process\nThis node is the bridge between the camera network and the physical AI model.","status":"open","priority":2,"issue_type":"task","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-22T03:18:26Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-22T03:18:26Z","dependencies":[{"issue_id":"turtlebot-maze-0tj","depends_on_id":"turtlebot-maze-3a5","type":"blocks","created_at":"2026-02-21T23:49:54Z","created_by":"Pantelis Monogioudis","metadata":"{}"},{"issue_id":"turtlebot-maze-0tj","depends_on_id":"turtlebot-maze-qob","type":"blocks","created_at":"2026-02-21T23:49:54Z","created_by":"Pantelis Monogioudis","metadata":"{}"}]}
{"id":"turtlebot-maze-3a5","title":"Design agent-environment communication interface","description":"Define the ROS 2 topic/service API between the robot agent and the environment agent. This includes:\n- Topic schema for environment observations sent to the robot (e.g., global pose corrections, obstacle map updates, suggested waypoints)\n- Topic schema for robot state published to the environment (e.g., intended goals, local sensor data, action status)\n- Service interfaces for on-demand queries (e.g., 'where am I?', 'is path clear to X?')\n- Message type definitions (custom ROS 2 msgs/srvs)\n- Latency and QoS requirements for real-time guidance\nThis is the foundation that all other sub-issues depend on.","status":"open","priority":2,"issue_type":"task","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-22T03:18:17Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-22T03:18:17Z"}
{"id":"turtlebot-maze-4y3","title":"Author tutorial blog post: Enhancing the TurtleBot Maze World","description":"Write a Mintlify blog post tutorial covering all steps to enhance the maze world: cherry-picking assets from PR #3, creating sim_house_enhanced.sdf.xacro with taller walls and PBR textures, adding ArUco marker spawner, parameterizing launch files with world_name/use_aruco, and adding demo-world-enhanced Docker Compose service. Target location: /home/pantelis.monogioudis/local/web/sites/courses/eaia/src/blog/tutorials/","status":"closed","priority":2,"issue_type":"task","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-20T16:47:38Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-20T20:05:38Z","closed_at":"2026-02-20T20:05:38Z","close_reason":"Tutorial rewritten: starts from original featureless maze, walks through generic enhancement steps, no PR references. Pushed to eaia PR #46."}
{"id":"turtlebot-maze-5k3","title":"Build ROS 2 Documentation MCP Server","description":"Roboticists using Claude Code (or any MCP-compatible AI assistant) lack a documentation MCP server for ROS 2, Gazebo Sim, and Nav2. When verifying API behavior (e.g., how ExecuteProcess handles nested lists, what launch arguments gz_spawn_model accepts), the only options are web search or reading installed source — neither is reliable or fast.\n\n## Goal\nBuild an MCP server that indexes and serves ROS 2 ecosystem documentation, enabling AI assistants to look up API references, launch file parameters, message types, and library behavior on demand.\n\n## Scope\n- Index docs.ros.org (launch API, rclpy, rclcpp, message/service/action types)\n- Index Gazebo Sim API docs (gz-sim, SDFormat, gz-transport)\n- Index Nav2 documentation (configuration, plugins, behavior trees)\n- Expose tools: search-docs, get-api-reference, get-message-type, get-launch-args\n- Package as installable MCP server for Claude Code\n\n## Existing Options to Evaluate\n1. **Context7 MCP** (github.com/upstash/context7) — general-purpose library doc server, may already cover some ROS 2 libs\n2. **Library Docs MCP Server** (vikramdse/docs-mcp-server) — generic doc search via Serper API\n3. **Google Developer Knowledge MCP** — pattern reference for doc-serving MCP\n4. **Custom build** — scrape/index ROS 2 doc sites, serve via MCP tools\n\n## Deliverables\n1. Working ROS 2 docs MCP server (MVP: launch API + Nav2 config docs)\n2. Integration test with Claude Code\n3. Blog tutorial in eaia repo (src/blog/tutorials/) documenting the problem, surveying existing options, and showing the solution","design":"# ROS 2 Documentation MCP Server — Design Outline\n\n## Problem Statement\nAI coding assistants (Claude Code, Cursor, etc.) connected to ROS 2 systems via ros-mcp-server can introspect running robots but cannot consult ROS 2 documentation. When a roboticist needs to verify API behavior — e.g., how ExecuteProcess.cmd handles LaunchConfiguration substitutions, or what arguments gz_spawn_model.launch.py accepts — the assistant must fall back to web search or reading raw source code. This is slow, unreliable, and breaks the workflow.\n\n## Architecture\n- Python MCP server using FastMCP or the official MCP Python SDK\n- Documentation ingested from public sources, chunked and indexed\n- Vector or keyword search over indexed docs\n- Tools exposed: search_ros2_docs, get_api_reference, get_message_definition, get_launch_file_args\n\n## Documentation Sources (Priority Order)\n1. **ROS 2 Launch API** — docs.ros.org/en/jazzy/p/launch/\n2. **rclpy API** — docs.ros.org/en/jazzy/p/rclpy/\n3. **Nav2 docs** — docs.nav2.org\n4. **SDFormat spec** — sdformat.org\n5. **Gazebo Sim API** — gazebosim.org/api\n6. **ROS 2 tutorials** — docs.ros.org/en/jazzy/Tutorials/\n\n## Existing Options Survey\n| Option | Pros | Cons |\n|--------|------|------|\n| Context7 MCP | Ready-made, curated DB, Claude plugin | May not cover ROS 2; proprietary index |\n| Library Docs MCP (vikramdse) | Generic, uses Serper search | External API dependency; not ROS-specific |\n| Google Dev Knowledge MCP | Good pattern reference | Google docs only |\n| Custom build | Full control, offline-capable, ROS 2-specific | Build effort |\n\n## Implementation Plan\n### Phase 1: Evaluate Context7 coverage\n- Install Context7 MCP server\n- Test if ROS 2 launch, rclpy, Nav2 docs are indexed\n- If coverage is sufficient, document setup and skip custom build\n\n### Phase 2: Custom MCP server (if needed)\n- Scrape/download ROS 2 doc pages (HTML → markdown)\n- Chunk and index with lightweight search (e.g., tantivy, sqlite-fts5, or embedded vectors)\n- Implement MCP tools: search_ros2_docs, get_api_reference\n- Package as pip-installable, add to Claude Code via claude mcp add\n\n### Phase 3: Blog tutorial\n- Write tutorial in eaia repo: src/blog/tutorials/ros2-docs-mcp/\n- Document the problem (roboticists + AI assistants + missing docs)\n- Survey all options (Context7, Library Docs MCP, Google Dev Knowledge, custom)\n- Show working setup with Claude Code\n- Include practical examples (verifying launch API behavior, looking up message types)","notes":"# Landscape: Documentation MCP Servers for Roboticists\n\n## The Gap\n\nRoboticists using Claude Code with ros-mcp-server can introspect a running ROS 2 system (topics, services, actions, parameters) but have no way to consult API documentation. When verifying API behavior — e.g., how `ExecuteProcess.cmd` handles nested lists with `LaunchConfiguration` substitutions, or what arguments `gz_spawn_model.launch.py` accepts — the only options are:\n\n1. **Web search** — unreliable, slow, may return outdated results\n2. **Reading installed source code** — accurate but requires knowing where to look\n3. **Claude's training data** — covers ROS 2 through Jazzy but can be stale or wrong on edge cases\n\nNone of these are fast or integrated into the AI assistant workflow.\n\n## Existing Options\n\n### 1. Context7 MCP Server (Most Promising)\n- **Repo**: https://github.com/upstash/context7\n- **What it does**: Serves up-to-date, version-specific library documentation directly into the LLM context window. Pulls docs and code examples from a curated database.\n- **Install for Claude Code**: `claude mcp add --scope user context7 -- npx -y @upstash/context7-mcp`\n- **Tools**: `resolve-library-id` (find library), `query-docs` (fetch docs for a library)\n- **ROS 2 coverage**: Unknown — needs evaluation. Libraries can potentially be added to their index.\n- **Pros**: Ready-made, curated, version-aware, Claude plugin available\n- **Cons**: Proprietary index; may not cover ROS 2 ecosystem; requires npm/npx\n\n### 2. Library Docs MCP Server\n- **Registry**: https://lobehub.com/mcp/vikramdse-docs-mcp-server\n- **What it does**: Generic MCP server for searching and fetching documentation for popular libraries using the Serper API.\n- **Pros**: Generic approach works for any library with online docs\n- **Cons**: External API dependency (Serper); not ROS-specific; search quality depends on web indexing\n\n### 3. Google Developer Knowledge MCP\n- **Announcement**: https://developers.googleblog.com/introducing-the-developer-knowledge-api-and-mcp-server/\n- **What it does**: Serves Google's official developer documentation via MCP. Provides a canonical, machine-readable gateway.\n- **Pros**: Shows the gold-standard pattern for doc-serving MCP; structured API\n- **Cons**: Google docs only — not applicable to ROS 2 directly; useful as architecture reference\n\n### 4. Custom ROS 2 Docs MCP Server (Build It)\n- **What it would do**: Scrape/index docs.ros.org, Nav2, Gazebo Sim, SDFormat docs. Expose search and lookup tools via MCP.\n- **Target doc sources**: launch API, rclpy, rclcpp, message types, Nav2 config, SDFormat spec, Gazebo Sim API\n- **Tools**: `search_ros2_docs`, `get_api_reference`, `get_message_definition`, `get_launch_file_args`\n- **Pros**: Full control, offline-capable, ROS 2-specific, covers the full ecosystem\n- **Cons**: Build and maintenance effort; need to handle doc updates across ROS 2 releases\n\n## Recommendation\n\nStart with Phase 1: evaluate Context7 coverage of ROS 2. If it covers rclpy/launch/Nav2, document setup and skip custom build. If not, proceed to Phase 2 (custom build) using Google Dev Knowledge MCP as architecture reference.","status":"open","priority":2,"issue_type":"feature","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-21T11:54:08Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-21T11:55:58Z"}
{"id":"turtlebot-maze-8eg","title":"Dev container workspace path /workspaces/turtlebot-maze does not exist","description":"When launching the Docker dev container via VS Code (Dev Container: Existing Docker Compose), two errors occur: (1) VS Code dialog: Workspace does not exist - Please select another workspace to open. (2) Terminal error: The terminal process failed to launch: Starting directory (cwd) /workspaces/turtlebot-maze does not exist. The devcontainer.json or docker-compose config references /workspaces/turtlebot-maze as the workspace folder, but that path is not created or mounted inside the container. Fix: Ensure the volume mount in docker-compose.yml maps the project source to /workspaces/turtlebot-maze, or update devcontainer.json workspaceFolder to match the actual mount point.","notes":"Root cause: Two issues - (1) devcontainer.json pointed to wrong service (demo-world → dev) and volume mount path was incorrect, (2) zenoh-bridge-ros2dds v1.2.1 .deb no longer exists on GitHub (repo renamed to zenoh-plugin-ros2dds, releases now ship as zip archives). Fix: Updated .devcontainer files to use dev service with correct mount, and updated Dockerfile.gpu to use zenoh-plugin-ros2dds v1.7.2 with new zip-based download format.","status":"closed","priority":1,"issue_type":"bug","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-19T13:28:10Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-19T13:58:57Z","closed_at":"2026-02-19T13:58:57Z","close_reason":"All three issues fixed and verified: (1) devcontainer service pointed to dev, (2) zenoh-bridge updated to v1.7.2 from new repo, (3) only install bridge deb to avoid zenohd dependency. Docker build succeeds and container starts with correct workspace mount."}
{"id":"turtlebot-maze-b2j","title":"Test ArUco marker detection with camera","description":"Verify that the ArUco markers (IDs 60 and 80) spawned in demo-world-enhanced are visible and detectable by the TurtleBot's camera. Drive the robot to marker locations and confirm detection using OpenCV's ArUco detector or a simple subscriber. This validates the markers placed by aruco_marker_spawner.launch.py are correctly positioned and oriented for camera-based pose estimation.","status":"closed","priority":2,"issue_type":"task","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-22T00:25:26Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-22T01:54:50Z","closed_at":"2026-02-22T01:54:50Z","close_reason":"ArUco marker detection verified working. DICT_4X4_100 detects marker ID 80 at 98% rate (53/54 frames). Fixed model SDFs to use flat plane PBR textures, corrected marker positions inside house bounds, updated test script."}
{"id":"turtlebot-maze-dwg","title":"Demo: navigate robot to desired location via ros-mcp-server","description":"Create a demo showcasing ros-mcp-server's ability to command the TurtleBot to navigate to a user-specified location using Nav2.\n\n## Context\nThe ros-mcp-server is already integrated (.mcp.json, rosbridge service on port 9090). The simulation world (sim_house) has 4 predefined locations in sim_house_locations.yaml:\n- location1: (-1.0, -0.5, -2.25)\n- location2: (0.5, 4.0, 0.785)\n- location3: (4.0, 0.5, 1.571)\n- location4: (2.75, 2.5, -1.571)\nRobot starts at (0.6, 0.6, 0.0).\n\n## Demo Flow\n1. Start the simulation: `docker compose up demo-world`\n2. Start rosbridge: `docker compose up rosbridge`\n3. Use ros-mcp-server tools from Claude Code to:\n   a. `connect_to_robot` (127.0.0.1:9090)\n   b. `get_topics` / `get_actions` to verify Nav2 is running\n   c. `subscribe_once` to /amcl_pose to confirm robot's current position\n   d. `send_action_goal` on /navigate_to_pose (nav2_msgs/action/NavigateToPose) to move the robot to a user-chosen location\n   e. `get_action_status` to monitor navigation progress\n   f. `subscribe_once` to /amcl_pose to confirm arrival\n\n## Permissions\nsend_action_goal is intentionally NOT pre-approved in .claude/settings.local.json — user must grant permission each time (safety gate for robot commands). Consider whether to add it to the allow list for the demo.\n\n## Acceptance Criteria\n- Document the demo steps in a script or README section\n- Verify the full flow works end-to-end in the sim_house world\n- Show the robot navigating from its start pose to at least one of the 4 known locations\n- Capture the ros-mcp-server tool calls and their responses as part of the demo documentation","status":"closed","priority":2,"issue_type":"feature","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-19T14:13:14Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-19T14:33:09Z","closed_at":"2026-02-19T14:33:09Z","close_reason":"Demo verified end-to-end: /navigate slash command connects to rosbridge, verifies Nav2, sends NavigateToPose goal to location3, robot arrives within 0.1m of target"}
{"id":"turtlebot-maze-iyw","title":"Fix broken Mermaid diagram in README System Overview","description":"The System Overview section in README.md contains a Mermaid diagram that fails to render on GitHub with a lexical error:\n\n```\nLexical error on line 3. Unrecognized text.\n...AM[/camera/image_raw] NAV[Nav2 S\n-----------------------^\n```\n\nGitHub's Mermaid renderer is choking on the diagram syntax — likely special characters in node labels (forward slashes in topic names like `/camera/image_raw`, or spaces/special chars in labels). Need to fix the Mermaid syntax so it renders correctly on GitHub.","status":"closed","priority":2,"issue_type":"bug","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-21T12:24:46Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-21T12:26:29Z","closed_at":"2026-02-21T12:26:29Z","close_reason":"Fixed in PR #10 — quoted all special characters in Mermaid node labels and subgraph titles. Auto-merge enabled."}
{"id":"turtlebot-maze-j6n","title":"Integrate physical AI / world model into environment agent","description":"Implement the physical AI model that the environment agent maintains:\n- Real-time digital twin of the maze state built from camera observations\n- Robot state estimation (position, velocity, heading) from overhead tracking\n- Dynamic occupancy grid updated from camera feeds (not just static map)\n- Object/obstacle tracking and prediction (if dynamic obstacles are introduced)\n- Evaluate approaches: classical CV + state estimation vs. learned world models (e.g., neural scene representations)\n- The model should provide the environment agent with predictive capability (e.g., 'robot will collide in N seconds at current trajectory')\n- Consider integration with NVIDIA Isaac or similar physical AI frameworks","status":"open","priority":2,"issue_type":"task","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-22T03:18:30Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-22T03:18:30Z","dependencies":[{"issue_id":"turtlebot-maze-j6n","depends_on_id":"turtlebot-maze-0tj","type":"blocks","created_at":"2026-02-21T23:49:55Z","created_by":"Pantelis Monogioudis","metadata":"{}"}]}
{"id":"turtlebot-maze-jww","title":"Decouple agent from environment with camera-network physical AI","description":"Decouple the robot agent from the environment so the environment becomes 'smart' — assisted by a separate agent that is aware of a physical AI model. The physical AI model is driven by a network of overhead cameras that observe the maze.\n\nKey architectural changes:\n- **Agent-environment decoupling**: The TurtleBot agent operates independently, receiving guidance/observations from the environment agent rather than relying solely on its onboard sensors\n- **Smart environment agent**: A new agent that processes the overhead camera network feeds and maintains a physical AI model (digital twin / world model) of the maze state\n- **Camera network**: Multiple fixed cameras overlooking the maze provide a bird's-eye or multi-angle view, enabling the environment agent to track robot position, obstacles, and maze layout\n- **Physical AI model**: The environment agent uses camera feeds to build/update a real-time physical model of the maze — this could include occupancy, object positions, dynamic obstacles, and robot state estimation\n- **Communication interface**: Define the API/topics between the environment agent and the robot agent (e.g., suggested waypoints, obstacle alerts, global localization corrections)\n\nThis enables scenarios where the robot has limited onboard sensing but benefits from an omniscient environment that can guide it through the maze.","status":"open","priority":2,"issue_type":"feature","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-22T02:59:19Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-22T02:59:19Z"}
{"id":"turtlebot-maze-m5e","title":"Test stella_vslam Visual SLAM with enhanced maze world","description":"PR #3 (oscarpoudel:updated_maze_plus_slam) includes an install script for stella_vslam at `docker/install_stella_vslam.sh`. The claim is that stella_vslam can run Visual SLAM on the TurtleBot in the maze world. We need to evaluate and integrate this.\n\n## Background\n- [stella_vslam](https://github.com/stella-cv/stella_vslam) is a Visual SLAM framework (fork of OpenVSLAM)\n- PR #3 includes a Jazzy-compatible ROS 2 wrapper: `oscarpoudel/stella_vslam_ros2_Jazzy`\n- The install script builds: iridescence (viewer), stella_vslam (core), iridescence_viewer, and the ROS 2 wrapper\n- The enhanced maze world (`demo-world-enhanced`) with textured walls should provide better visual features for VSLAM\n\n## Tasks\n\n### 1. Cherry-pick and evaluate the install script\n```bash\ngit checkout pr3-temp -- docker/install_stella_vslam.sh\n```\n- Review the script for correctness and security (hardcoded paths to `~/lib`, `~/ros2_ws`)\n- Check if `oscarpoudel/stella_vslam_ros2_Jazzy` repo exists and is maintained\n- Check stella_vslam compatibility with ROS 2 Jazzy and gz-sim\n\n### 2. Dockerize the build\n- The install script uses `~/lib` and `~/ros2_ws` — needs adaptation for our Docker multi-stage build\n- Consider a new Dockerfile stage or separate Dockerfile for stella_vslam\n- stella_vslam needs camera images — verify the Gazebo camera topic is compatible\n\n### 3. Test with enhanced maze world\n- Launch `demo-world-enhanced` (textured walls provide visual features for VSLAM)\n- Run stella_vslam with the TurtleBot's camera feed\n- Verify it can build a map and localize\n- Compare SLAM performance: original featureless walls vs enhanced textured walls\n\n### 4. Add Docker Compose service\n- Add `demo-slam` or similar service if stella_vslam works\n- Document launch procedure in README\n\n## References\n- PR #3: https://github.com/pantelis/turtlebot-maze/pull/3\n- stella_vslam: https://github.com/stella-cv/stella_vslam\n- Jazzy wrapper (claimed): https://github.com/oscarpoudel/stella_vslam_ros2_Jazzy\n- Install script in PR #3: `docker/install_stella_vslam.sh`","status":"closed","priority":2,"issue_type":"feature","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-21T12:36:45Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-21T13:11:07Z","closed_at":"2026-02-21T13:11:07Z","close_reason":"PR #12 implements standalone stella_vslam Visual SLAM demo with Zenoh transport. Docker build + manual testing needed."}
{"id":"turtlebot-maze-peq","title":"End-to-end integration test: environment-assisted maze navigation","description":"Create a full integration test that validates the decoupled architecture:\n- Launch enhanced world with overhead cameras + robot\n- Start environment agent with physical AI model\n- Start robot agent configured to accept environment guidance\n- Robot navigates the maze using a combination of onboard sensing and environment guidance\n- Verify: environment agent correctly tracks robot, provides accurate observations\n- Verify: robot agent uses environment data to improve navigation (fewer collisions, faster completion)\n- Compare performance metrics: standalone vs environment-assisted navigation\n- Docker compose service definitions for the full stack","status":"open","priority":2,"issue_type":"task","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-22T03:18:38Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-22T03:18:38Z","dependencies":[{"issue_id":"turtlebot-maze-peq","depends_on_id":"turtlebot-maze-j6n","type":"blocks","created_at":"2026-02-21T23:49:55Z","created_by":"Pantelis Monogioudis","metadata":"{}"},{"issue_id":"turtlebot-maze-peq","depends_on_id":"turtlebot-maze-t84","type":"blocks","created_at":"2026-02-21T23:49:55Z","created_by":"Pantelis Monogioudis","metadata":"{}"}]}
{"id":"turtlebot-maze-qob","title":"Set up overhead camera network in Gazebo","description":"Add a network of fixed cameras overlooking the maze in the Gazebo simulation:\n- Determine optimal camera placement for full maze coverage (bird's-eye and/or multi-angle)\n- Add camera sensor models to sim_house_enhanced.sdf.xacro (or a new world variant)\n- Configure camera parameters (resolution, FOV, frame rate) appropriate for tracking\n- Publish camera feeds on ROS 2 topics (one per camera)\n- Verify full maze coverage with no blind spots\n- Consider both single overhead camera and multi-camera configurations\nStart with simulation; physical camera setup is a separate future task.","status":"open","priority":2,"issue_type":"task","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-22T03:18:23Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-22T03:18:23Z","dependencies":[{"issue_id":"turtlebot-maze-qob","depends_on_id":"turtlebot-maze-3a5","type":"blocks","created_at":"2026-02-21T23:49:54Z","created_by":"Pantelis Monogioudis","metadata":"{}"}]}
{"id":"turtlebot-maze-t84","title":"Refactor robot agent to accept environment guidance","description":"Modify the existing TurtleBot autonomy stack to operate in the decoupled architecture:\n- Subscribe to environment agent observations (global pose, waypoint suggestions, obstacle alerts)\n- Integrate environment data into Nav2 planning (e.g., as an external costmap layer or goal source)\n- Support fallback to onboard-only sensing when environment agent is unavailable\n- Behavior tree modifications: add conditions/actions for environment-assisted navigation\n- The robot should still function autonomously but perform better with environment assistance\n- Maintain backward compatibility with current standalone demo","status":"open","priority":2,"issue_type":"task","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-22T03:18:34Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-22T03:18:34Z","dependencies":[{"issue_id":"turtlebot-maze-t84","depends_on_id":"turtlebot-maze-3a5","type":"blocks","created_at":"2026-02-21T23:49:54Z","created_by":"Pantelis Monogioudis","metadata":"{}"}]}
{"id":"turtlebot-maze-uqk","title":"Investigate missing RealSense camera frames in RViz2","description":"Cannot see frames from the RealSense camera in RViz2. Need to diagnose why — likely a TF, topic, or driver configuration issue.","notes":"Fix verified: all 3 ROS topics (/intel_realsense_r200_depth/image_raw, /points, /camera/image_raw) now have data flowing at ~5Hz. Root cause was sensor type=camera (needed type=depth) and bridge subscribed to wrong Gazebo topic names. The depth_image subtopic does not publish with type=depth, but the base topic carries 32FC1 depth data which is sufficient.","status":"closed","priority":2,"issue_type":"bug","owner":"pantelis.monogioudis@aegean.ai","created_at":"2026-02-19T14:53:59Z","created_by":"Pantelis Monogioudis","updated_at":"2026-02-19T16:52:09Z","closed_at":"2026-02-19T16:52:09Z","close_reason":"Fixed: changed sensor type from camera to depth (matching upstream nav2_minimal_tb3_sim), corrected bridge topic mapping to use base Gazebo topic for images. PR #7 verified with data flowing on all expected topics at 5Hz."}
