# Docker Compose file for TurtleBot Behavior Examples
#
# Usage:
#
# To build the images:
#   docker compose build
#
# To start up a specific service by name:
#   docker compose up <service_name>
#
# To open an interactive shell to a running container:
#   docker exec -it <container_name> bash

services:
  # Base image containing dependencies.
  base:
    image: turtlebot_behavior:base
    build:
      context: .
      dockerfile: docker/Dockerfile.gpu
      args:
        ROS_DISTRO: ${ROS_DISTRO:?}
      target: base
    # Interactive shell
    stdin_open: true
    tty: true
    # Networking and IPC for ROS 2
    network_mode: host
    ipc: host
    # Needed to display graphical applications
    privileged: True
    environment:
      # Needed to define a TurtleBot model type (3 or 4)
      - TURTLEBOT_MODEL=${TURTLEBOT_MODEL:-3}
      # Allows graphical programs in the container.
      # - __GLX_VENDOR_LIBRARY_NAME=nvidia
      - NVIDIA_VISIBLE_DEVICES=all
      - DISPLAY=${DISPLAY}
      - QT_X11_NO_MITSHM=1
      - NVIDIA_DRIVER_CAPABILITIES=all
    volumes:
      # Allows graphical programs in the container.
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      - ${XAUTHORITY:-$HOME/.Xauthority}:/root/.Xauthority

  # Overlay image containing the example source code.
  overlay:
    extends: base
    image: turtlebot_behavior:overlay
    build:
      context: .
      dockerfile: docker/Dockerfile.gpu
      target: overlay

  # Developer container
  dev:
    extends: overlay
    image: turtlebot_behavior:dev
    build:
      context: .
      dockerfile: docker/Dockerfile.gpu
      target: dev
      args:
        - UID=${UID:-1000}
        - GID=${UID:-1000}
        - USERNAME=${USERNAME:-devuser}
    volumes:
      # Mount the source code
      - ./tb_autonomy:/overlay_ws/src/tb_autonomy:rw
      - ./tb_worlds:/overlay_ws/src/tb_worlds:rw
      # Mount colcon build artifacts for faster rebuilds
      - ./.colcon/build/:/overlay_ws/build/:rw
      - ./.colcon/install/:/overlay_ws/install/:rw
      - ./.colcon/log/:/overlay_ws/log/:rw
    user: ${USERNAME:-devuser}
    command: sleep infinity

  # Demo simulation world
  demo-world:
    extends: overlay
    command: ros2 launch tb_worlds tb_demo_world.launch.py

  # Enhanced demo world with taller textured walls and ArUco markers
  demo-world-enhanced:
    extends: overlay
    environment:
      - GZ_SIM_RESOURCE_PATH=/overlay_ws/install/tb_worlds/share/tb_worlds/worlds:/overlay_ws/install/tb_worlds/share/tb_worlds/models
    command: >
      ros2 launch tb_worlds tb_demo_world.launch.py
      world_name:=sim_house_enhanced.sdf.xacro
      use_aruco:=True

  # Behavior demo using Python and py_trees
  demo-behavior-py:
    extends: overlay
    command: >
      ros2 launch tb_autonomy tb_demo_behavior_py.launch.py
      tree_type:=${BT_TYPE:?}
      enable_vision:=${ENABLE_VISION:?}
      target_color:=${TARGET_COLOR:?}
      detector_type:=${DETECTOR_TYPE:-hsv}
      target_object:=${TARGET_OBJECT:-cup}

  # Behavior demo using C++ and BehaviorTree.CPP
  demo-behavior-cpp:
    extends: overlay
    command: >
      ros2 launch tb_autonomy tb_demo_behavior_cpp.launch.py
      tree_type:=${BT_TYPE:?}
      enable_vision:=${ENABLE_VISION:?}
      target_color:=${TARGET_COLOR:?}
      detector_type:=${DETECTOR_TYPE:-hsv}
      target_object:=${TARGET_OBJECT:-cup}

  # Foxglove WebSocket bridge — connects app.foxglove.dev to all ROS 2 topics
  # Open https://app.foxglove.dev, choose "Open connection", WebSocket,
  # enter ws://localhost:8765
  foxglove-bridge:
    extends: overlay
    command: ros2 launch foxglove_bridge foxglove_bridge_launch.xml port:=8765
    ports:
      - "8765:8765"

  # Rosbridge WebSocket server for ros-mcp-server integration
  rosbridge:
    extends: overlay
    command: ros2 launch rosbridge_server rosbridge_websocket_launch.xml
    ports:
      - "9090:9090"

  # Zenoh router for cross-container pub/sub.
  # Loads zenoh/zenoh-storage.json5 which enables the storage-manager plugin
  # with an in-memory storage for tb/detections (queryable via get() within
  # the session). REST admin API available at http://localhost:8000.
  zenoh-router:
    image: eclipse/zenoh:latest
    network_mode: host
    ipc: host
    volumes:
      - ./zenoh:/config:ro
    command: ["--config", "/config/zenoh-storage.json5"]

  # Persistent detection logger — subscribes to tb/detections and appends
  # timestamped JSONL records to ./data/detections/detections.jsonl.
  # Run alongside the detector service for dataset generation:
  #   docker compose up zenoh-router zenoh-bridge detector detection-logger
  detection-logger:
    build:
      context: .
      dockerfile: docker/Dockerfile.torch.gpu
    network_mode: host
    ipc: host
    volumes:
      - ./detector:/app
      - ./data/detections:/data/detections
    working_dir: /app
    command: >
      python zenoh_logger.py
      --key tb/detections
      --connect tcp/localhost:7447
      --output /data/detections/detections.jsonl

  # Run detection logging tests inside the container (has eclipse-zenoh + pytest).
  # Usage: docker compose run --rm test-detection-logging
  test-detection-logging:
    build:
      context: .
      dockerfile: docker/Dockerfile.torch.gpu
    network_mode: host
    ipc: host
    volumes:
      - ./detector:/app
      - ./zenoh:/zenoh:ro
    working_dir: /app
    command: pytest test_detection_logging.py -v

  # SLAM pose logger — subscribes to tb/slam/pose and tb/slam/status.
  # Run alongside demo-slam for trajectory dataset generation:
  #   docker compose up zenoh-router demo-slam slam-logger
  slam-logger:
    build:
      context: .
      dockerfile: docker/Dockerfile.torch.gpu
    network_mode: host
    ipc: host
    volumes:
      - ./detector:/app
      - ./data/slam:/data/slam
    working_dir: /app
    command: >
      python zenoh_logger.py
      --key tb/slam/**
      --connect tcp/localhost:7447
      --output /data/slam/slam.jsonl

  # Zenoh DDS bridge (exposes ROS 2 DDS topics as Zenoh keys with rt/ prefix)
  zenoh-bridge:
    extends: overlay
    network_mode: host
    ipc: host
    command: zenoh-bridge-ros2dds -e tcp/localhost:7447 client

  # PyTorch YOLOv8 object detector (subscribes via Zenoh, no ROS 2 needed)
  detector:
    build:
      context: .
      dockerfile: docker/Dockerfile.torch.gpu
    network_mode: host
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./detector:/app
    working_dir: /app
    command: python object_detector.py --confidence 0.5 --model yolov8n.pt --connect tcp/localhost:7447

  # stella_vslam Visual SLAM (standalone, Zenoh transport)
  demo-slam:
    build:
      context: .
      dockerfile: docker/Dockerfile.slam
    network_mode: host
    ipc: host
    environment:
      - DISPLAY
      - PYTHONUNBUFFERED=1
      - ZENOH_ROUTER=tcp/localhost:7447
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
    command: >
      --image-key intel_realsense_r200_depth/image_raw
      --depth-key intel_realsense_r200_depth/depth/image_raw
      --config /slam/config/turtlebot_realsense.yaml
      --vocab /slam/vocab/orb_vocab.fbow
